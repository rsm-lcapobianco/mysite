[
  {
    "objectID": "Personnel/Reddit_Sentiment/Reddit_Scrape.html",
    "href": "Personnel/Reddit_Sentiment/Reddit_Scrape.html",
    "title": "Begin sentiment Analysis",
    "section": "",
    "text": "import praw\nimport regex as re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport os\nimport sklearn\nimport nltk\n\nMatplotlib created a temporary cache directory at /tmp/matplotlib-rtvczs6a because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\nclientid = ''\nclient_secret = ''\nuser_agent = \"\"\nreddit = praw.Reddit(\n    client_id=clientid,\n    client_secret=client_secret,\n    user_agent=user_agent\n)\nprint(reddit.read_only)\n# Output: True\n\nTrue\nimport re\n\ndef find_matched_terms(text, terms):\n    matched = []\n    for term in terms:\n        # Word-boundary search (e.g., matches \"lead\" but not \"leadership\")\n        pattern = rf'\\b{re.escape(term)}\\b'\n        if re.search(pattern, text, flags=re.IGNORECASE):\n            matched.append(term)\n    return matched\ntar_sub = 'datacenter'\nbattery_terms = ['battery', 'batteries', 'lithium', 'li-ion', 'sodium', 'lead', 'acid', 'ups','sodium','ion']\ntime_filter = 'all'\ndata = []\nseen_comments = set()\n\nfor term in battery_terms:\n    for submission in reddit.subreddit(tar_sub).search(term, sort=\"top\", time_filter=time_filter):\n        submission.comments.replace_more(limit=0)\n        for comment in submission.comments.list():\n            if comment.id in seen_comments:\n                continue\n            text = comment.body.lower()\n            matched_terms = find_matched_terms(text, battery_terms)\n            if matched_terms:\n                data.append({\n                    'comment_id': comment.id,\n                    'author': str(comment.author),\n                    'text': text,\n                    'matched_terms': matched_terms,\n                    'submission_id': submission.id,\n                    'submission_title': submission.title\n                })\n                seen_comments.add(comment.id)\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n1\ng0diub1\nRedebo\ndisconnect the battery strings via the associa...\n[battery, batteries, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n2\ng0dtxqg\nletsbebuns\nthe batteries should be removed based on age. ...\n[batteries, lead, acid, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n3\ng0ebpdx\nlooktowindward\nyes, they are a safety hazard. you should call...\n[battery]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n4\ng0euygg\nxpkranger\ni’m going to get rid of them, waiting on a quo...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nexample = df['text'][1]\nprint(example)\n\nex_analyzer = SentimentIntensityAnalyzer()\n\nex_sent = ex_analyzer.polarity_scores(example)\nprint(ex_sent)\n\ndisconnect the battery strings via the associated battery breaker or switch that is either in the ups or electrically in line with the ups.  if you want to be safer after that, remove the intercell jumpers between each battery (caution, batteries are always 'live' and can present voltage and current when both terminals are touched).  once the batteries are disconnected and just sitting on a shelf, you can leave them there practically forever with no risk.\n\nalso, would definitely recommend rip and replace on a 15 year old ups.  if you are located in the western us, my company can assist if you're in need.\n{'neg': 0.011, 'neu': 0.889, 'pos': 0.1, 'compound': 0.8366}\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nanalyzer = SentimentIntensityAnalyzer()\n# Apply VADER to each comment's text\ndf['sentiment'] = df['text'].apply(lambda x: analyzer.polarity_scores(x))\ndf = pd.concat([df.drop('sentiment', axis=1), df['sentiment'].apply(pd.Series)], axis=1)\ndef get_label(score):\n    if score &gt;= 0.05:\n        return 'positive'\n    elif score &lt;= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\ndf['sentiment_label'] = df['compound'].apply(get_label)\ndf.head(3)\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\nneg\nneu\npos\ncompound\nsentiment_label\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.084\n0.873\n0.043\n-0.4215\nnegative\n\n\n1\ng0diub1\nRedebo\ndisconnect the battery strings via the associa...\n[battery, batteries, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.011\n0.889\n0.100\n0.8366\npositive\n\n\n2\ng0dtxqg\nletsbebuns\nthe batteries should be removed based on age. ...\n[batteries, lead, acid, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.029\n0.971\n0.000\n-0.2617\nnegative\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Simulate a sample of the df for visualization\ndf_viz = pd.DataFrame({\n    'matched_terms': df['matched_terms'],\n    'sentiment_label': df['sentiment_label']\n})\ndf_exploded = df_viz.explode('matched_terms')\n\n\n# Explode matched_terms to allow grouping by single term\ndf_exploded = df_viz.explode('matched_terms')\n\n# Count sentiment labels per battery type\nsentiment_counts = df_exploded.groupby(['matched_terms', 'sentiment_label']).size().unstack(fill_value=0)\n\n# Plot the sentiment distribution per battery type\nsentiment_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Sentiment Distribution by Battery Type')\nplt.xlabel('Battery Type')\nplt.ylabel('Number of Comments')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.grid(axis='y')\n\nplt.show()\ndf\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\nneg\nneu\npos\ncompound\nsentiment_label\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.084\n0.873\n0.043\n-0.4215\nnegative\n\n\n1\ng0diub1\nRedebo\ndisconnect the battery strings via the associa...\n[battery, batteries, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.011\n0.889\n0.100\n0.8366\npositive\n\n\n2\ng0dtxqg\nletsbebuns\nthe batteries should be removed based on age. ...\n[batteries, lead, acid, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.029\n0.971\n0.000\n-0.2617\nnegative\n\n\n3\ng0ebpdx\nlooktowindward\nyes, they are a safety hazard. you should call...\n[battery]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.000\n0.718\n0.282\n0.6705\npositive\n\n\n4\ng0euygg\nxpkranger\ni’m going to get rid of them, waiting on a quo...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.000\n0.807\n0.193\n0.7351\npositive\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n263\nfrhl2ew\nswedishhat\nto be fair, full 19in eia racks with ups's are...\n[ups]\nfemyhi\nDoes anyone here use OCP hardware in their dat...\n0.000\n0.944\n0.056\n0.7391\npositive\n\n\n264\nkih1o5a\nNone\n&gt; finally adding your future generator? ah ha ...\n[lead]\n199drs1\nHow can a Field Controls Engineer get PLC Prog...\n0.056\n0.892\n0.052\n-0.1779\nnegative\n\n\n265\ng9583fq\nRedebo\nyou might ask your provider if they have some ...\n[battery]\njcvdpk\nFloor protection in data center\n0.000\n0.940\n0.060\n0.2732\npositive\n\n\n266\nmmhd66m\nAlligatorDan\nthe engineering mindset on youtube is a great ...\n[ups]\n1jw4mql\nAmazon DCEO\n0.013\n0.827\n0.160\n0.9764\npositive\n\n\n267\nmmgzvfn\nLucky_Luciano73\namazon has hired ex-navy guys that we work wit...\n[ups]\n1jw4mql\nAmazon DCEO\n0.092\n0.718\n0.190\n0.6486\npositive\n\n\n\n\n268 rows × 11 columns"
  },
  {
    "objectID": "Personnel/Reddit_Sentiment/Reddit_Scrape.html#model-is-being-trained-on-the-twitter-data",
    "href": "Personnel/Reddit_Sentiment/Reddit_Scrape.html#model-is-being-trained-on-the-twitter-data",
    "title": "Begin sentiment Analysis",
    "section": "model is being trained on the twitter data",
    "text": "model is being trained on the twitter data\n\nMODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\n\nencoded_text = tokenizer(example,return_tensors='pt')\noutput = model(**encoded_text)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n\nscores_dict = {\n    'roberta_neg': scores[0],\n    'roberta_neutral': scores[1],\n    'roberta_post': scores[2]\n}\n\n\nprint(scores_dict)\nprint(ex_sent)\n\n{'roberta_neg': 0.07787177, 'roberta_neutral': 0.6052092, 'roberta_post': 0.31691906}\n{'neg': 0.011, 'neu': 0.889, 'pos': 0.1, 'compound': 0.8366}\n\n\n\nfrom scipy.special import softmax\n\ndef polarity_scores_roberta(text):\n    encoded_text = tokenizer(\n        text,\n        return_tensors='pt',\n        truncation=True,       # THIS is what prevents the crash\n        max_length=512,\n        padding=True\n    )\n    output = model(**encoded_text)\n    scores = output[0][0].detach().numpy()\n    scores = softmax(scores)\n    labels = ['negative', 'neutral', 'positive']\n    return labels[scores.argmax()], scores.max()\n\n\ndf[['roberta_label', 'roberta_score']] = df['text'].apply(\n    lambda x: pd.Series(polarity_scores_roberta(x))\n)\n\n\ndf.head(1)\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\nneg\nneu\npos\ncompound\nsentiment_label\nroberta_label\nroberta_score\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.084\n0.873\n0.043\n-0.4215\nnegative\nnegative\n0.458596\n\n\n\n\n\n\n\n\ndf_exploded = df.explode('matched_terms')\n\n\n# Group by label to visualize distribution\nlabel_counts = df['roberta_label'].value_counts()\n\n# Plot sentiment label distribution\nplt.figure(figsize=(6, 4))\nlabel_counts.plot(kind='bar', color='skyblue')\nplt.title(\"RoBERTa Sentiment Label Distribution\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n# Plot average sentiment score by battery type\navg_score_by_term = df_exploded.groupby('matched_terms')['roberta_score'].mean().sort_values()\n\nplt.figure(figsize=(8, 5))\navg_score_by_term.plot(kind='barh', color='salmon')\nplt.title(\"Average RoBERTa Sentiment Score by Battery Term\")\nplt.xlabel(\"Average Score\")\nplt.ylabel(\"Battery Term\")\nplt.grid(axis='x')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ---------------------------\n# Visualization by Sentiment Class and Term\n# ---------------------------\nsentiment_counts = df_exploded.groupby(['matched_terms', 'roberta_label']).size().unstack(fill_value=0)\n\nplt.figure(figsize=(10, 6))\nsentiment_counts.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(10, 6))\nplt.title(\"Sentiment Distribution per Battery Term\")\nplt.xlabel(\"Battery Term\")\nplt.ylabel(\"Number of Comments\")\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n# ---------------------------\n# Drill-down: Most Positive & Negative per Term\n# ---------------------------\nmost_pos_comments = df_exploded.sort_values(by='roberta_score', ascending=False).groupby('matched_terms').first().reset_index()\nmost_neg_comments = df_exploded.sort_values(by='roberta_score').groupby('matched_terms').first().reset_index()\n\n# Combine for review\ndrill_df = pd.merge(\n    most_pos_comments[['matched_terms', 'text', 'roberta_score']],\n    most_neg_comments[['matched_terms', 'text', 'roberta_score']],\n    on='matched_terms',\n    suffixes=('_most_positive', '_most_negative')\n)\ndisplay(drill_df)\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_terms\ntext_most_positive\nroberta_score_most_positive\ntext_most_negative\nroberta_score_most_negative\n\n\n\n\n0\nacid\nactually, many ups', especially dc grade ups',...\n0.859889\n\"no such thing as a data center that's too col...\n0.455056\n\n\n1\nbatteries\nthat'll prove to be a disaster if the batterie...\n0.945713\nthis may have been the consensus 5 years ago.\\...\n0.436103\n\n\n2\nbattery\nare all of the raid batteries the same age? an...\n0.913489\ni handle all field ups units for a large truck...\n0.446345\n\n\n3\nion\nyes, it is detailed in their site that the mv ...\n0.865184\n\"no such thing as a data center that's too col...\n0.455056\n\n\n4\nlead\nfirst of all, taking on those data center move...\n0.944411\nit really depends op, working at a dc is great...\n0.407846\n\n\n5\nli-ion\nyes, it is detailed in their site that the mv ...\n0.865184\nof course it's feasible. tesla made mega batte...\n0.643169\n\n\n6\nlithium\nactually, many ups', especially dc grade ups',...\n0.859889\nthis may have been the consensus 5 years ago.\\...\n0.436103\n\n\n7\nups\nthe engineering mindset on youtube is a great ...\n0.973194\nyeah i wish we got more remote hands tickets a...\n0.426720\n\n\n\n\n\n\n\n\nneg_comments = df_exploded[df_exploded['roberta_label'] == 'negative']\nneg_lithium = neg_comments[neg_comments['matched_terms'] == 'lithium']\n\n\n\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')  # keeps only words, removes punctuation\n\ndef extract_keywords_no_punkt(text_series):\n    all_words = []\n    for text in text_series:\n        tokens = tokenizer.tokenize(text.lower())\n        filtered = [word for word in tokens if word not in stop_words]\n        all_words.extend(filtered)\n    return Counter(all_words).most_common(20)\n\n\n# Define stop_words using NLTK's built-in list\nfrom nltk.corpus import stopwords\n\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Now that stop_words is defined, rerun the visualization block\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndef extract_keywords_no_punkt(text_series):\n    all_words = []\n    for text in text_series:\n        tokens = tokenizer.tokenize(text.lower())\n        filtered = [word for word in tokens if word not in stop_words]\n        all_words.extend(filtered)\n    return Counter(all_words).most_common(20)\n\nchemistry_pain_points = {}\n\nbattery_terms = neg_comments['matched_terms'].unique()\n\nfor term in battery_terms:\n    chem_comments = neg_comments[neg_comments['matched_terms'] == term]\n    keywords = extract_keywords_no_punkt(chem_comments['text'])\n    chemistry_pain_points[term] = keywords\n\npain_point_data = []\n\nfor term, keywords in chemistry_pain_points.items():\n    for word, freq in keywords:\n        pain_point_data.append({'battery_chemistry': term, 'keyword': word, 'frequency': freq})\n\npain_point_df = pd.DataFrame(pain_point_data)\n\n# Visualize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntop_pain_points = (\n    pain_point_df\n    .sort_values(by='frequency', ascending=False)\n    .groupby('battery_chemistry')\n    .head(5)\n)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(\n    data=top_pain_points,\n    y='keyword',\n    x='frequency',\n    hue='battery_chemistry',\n    dodge=False\n)\nplt.title('Top Pain Point Keywords by Battery Chemistry')\nplt.xlabel('Frequency')\nplt.ylabel('Keyword')\nplt.legend(title='Battery Chemistry', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(axis='x')\nplt.show()\n\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Homework",
    "section": "",
    "text": "Hw 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nLowell Capobianco\nApr 18, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#introduction",
    "href": "projects/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was one of the largest of its kind. Notably, it was conducted for a politically motivated nonprofit using paper mail solicitations in 2005. These characteristics are important to consider, as donor behavior may have shifted in recent years. For example, individuals might become more politically active—and therefore more willing to donate—during contentious elections, while economic uncertainty could make donors more hesitant to give. Regardless of these contextual differences, we will explore how the researchers designed and analyzed their experiment, and use modern statistical techniques to replicate their findings."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#data",
    "href": "projects/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nFirst we’ll load the data from dta and conduct EDA inorder to better understand the datatypes and distrubtion of data.\nWe’ll rename the columns we want to make a distribtuion plot of some key columns and verify the distribution and check for any outliers.\n\n\nimport pandas as pd\nimport pyrsm as rsm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport statsmodels as sm\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#utllize Pandas and the Pyrsm package to create the distrubtion plots\ndata = pd.read_stata('data/karlan_list_2007.dta')\ndistr_plot_columns = ['treatment','gave','ask','hpa','ratio','median_hhincome','mrm2','dormant','female']\ndistr_data_plot = data[distr_plot_columns]\ndistr_data_plot.rename(columns={\n    'hpa': 'Highest Previous Amount',\n    'median_hhincome': 'Median Household Income',\n    'mrm2': 'Months Since Last Donation',\n    'gave': 'Has Previously Donated'\n}, inplace=True)\n\n\n\ndisplay(data.head(3))\n\n\nrsm.model.distr_plot(distr_data_plot)\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n\n\n3 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another. Looking at table 1 in the resarches paper we see reserachers have provided summary statistics of several features in the data. This allows us to see that the control and treatment group are split evenly among those features. Showing the split allows us to perform useful statics techniques.\nTo verify the success of random assignment, I compare pre-treatment characteristics between the treatment and control groups. Table 1 in the original study shows that the groups are similar across key variables, supporting the claim that any later differences are due to the treatment, not underlying differences.\nI replicate this by testing variables like mrm2 (months since last donation), dormant (donated earlier in 2005), and female. Both t-tests and linear regressions confirm no statistically significant differences, reinforcing that the groups are balanced and that the experimental design is valid.\n\n\n\n\n\n\nVariables To Test at the 95% CI\n\n\n\nmrm2, dormant,female\n\n\n\nWe’ll conduct a t-test with a 95% confidence interval to ensure and check the months since lost donation variable with a linear regression\n\nCreate a list of the variables we want to check\nCreate a function that we can use repedetly\nWe’ll use css and format the table for readability\n\n\n\nvariables_to_test = ['mrm2', 'dormant', 'female']\n\ndef t_test(data, target):\n    control_data = data[data['treatment'] == 0]\n    treatment_data = data[data['treatment'] == 1]\n\n    control_mean = control_data[target].mean()\n    treatment_mean = treatment_data[target].mean()\n\n    control_std = control_data[target].std()\n    treatment_std = treatment_data[target].std()\n\n    control_n = len(control_data[target].dropna())\n    treatment_n = len(treatment_data[target].dropna())\n\n    se = ((control_std**2 / control_n) + (treatment_std**2 / treatment_n)) ** 0.5\n    t_stat = (treatment_mean - control_mean) / se\n\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=control_n + treatment_n - 2))\n\n    significance = \"significant\" if p_value &lt; 0.05 else \"not significant\"\n    \n\n    res = pd.DataFrame([{\n        't_stat': round(t_stat, 2),\n        'control_mean': round(control_mean, 2),\n        'treatment_mean': round(treatment_mean, 2),\n        'mean_diff': round(treatment_mean - control_mean, 2),\n        'standard_error': round(se, 2),\n        'control_n': control_n,\n        'treatment_n': treatment_n,\n        'p_value':p_value,\n        'significance':significance\n    }])\n    return display(res)\nt_test_results = {}\n\nfrom IPython.display import display, HTML\n\nfor var in variables_to_test:\n    res = t_test(data, var)  # This displays a table already\n    html = f\"\"\"\n    &lt;div class ='balance-card'&gt;\n        &lt;h4&gt; {var} — Balance Test&lt;/h4&gt;\n        &lt;p&gt;This section shows a comparison between the treatment and control groups for &lt;code&gt;{var}&lt;/code&gt;. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.&lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n    display(HTML(html))\n    #\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n0.12\n13.0\n13.01\n0.01\n0.11\n16687\n33395\n0.904855\nnot significant\n\n\n\n\n\n\n\n\n    \n         mrm2 — Balance Test\n        This section shows a comparison between the treatment and control groups for mrm2. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.\n    \n    \n\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n0.17\n0.52\n0.52\n0.0\n0.0\n16687\n33396\n0.861961\nnot significant\n\n\n\n\n\n\n\n\n    \n         dormant — Balance Test\n        This section shows a comparison between the treatment and control groups for dormant. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.\n    \n    \n\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n-1.75\n0.28\n0.28\n-0.01\n0.0\n16339\n32633\n0.07952\nnot significant\n\n\n\n\n\n\n\n\n    \n         female — Balance Test\n        This section shows a comparison between the treatment and control groups for female. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.\n    \n    \n\n\n\n\nLinear Regression Results\nUsing a Linear regression we can verfy our results of the t-test\n\nreg_data= data[['mrm2','treatment']].dropna()\n\nx = reg_data[['treatment']]\ny= reg_data['mrm2']\n\nmodel= LinearRegression()\nmodel.fit(x,y)\n\n\nreg_df = pd.DataFrame({\n    \"Coefficient\": [\"Intercept\", \"Treatment Coefficient\"],\n    \"Estimate\": [round(model.intercept_, 4), round(model.coef_[0], 4)]\n})\n\ndisplay(reg_df)\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\n\n\n\n\n0\nIntercept\n12.9981\n\n\n1\nTreatment Coefficient\n0.0137"
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#experimental-results",
    "href": "projects/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nTo evaluate whether matching donations increase the likelihood that someone donates, I begin by comparing the share of donors between the treatment and control groups. A simple barplot shows a higher proportion of donations among individuals who received a match offer. This visual evidence suggests that the match has an encouraging effect on giving behavior.\nTo statistically test this difference, I run both a t-test and a simple linear regression using the binary outcome variable gave. Both methods show a small but statistically significant increase in donation likelihood for the treatment group—confirming the results reported in Table 2A, Panel A of the original paper. This indicates that simply announcing a matching offer increases the probability that someone donates, even if the amount of the match (e.g., 1:1 or 3:1) does not change that decision.\nI also fit a probit regression, which models the probability of donating using a nonlinear specification. While the coefficient is significant and consistent in sign with the linear model, the results do not numerically replicate Table 3 from the paper—likely due to rounding differences, omitted covariates, or reporting inconsistencies noted by the authors themselves. Nonetheless, the directional effect is clear: offering a match increases participation in giving.\nThese results reinforce a key behavioral insight: people are more likely to act charitably when they feel their gift is amplified, even if the actual match size doesn’t substantially change the outcome.\n\n\n\n\n\n\nPercentage of Donators by Treatment Group\n\n\n\n\n\n\n\nct = pd.crosstab(data['treatment'],data['gave'],normalize='index')\nct.index = ['Control', 'Treatment']\nct.columns = ['Did Not Give', 'Gave']\ndisplay(ct)\nct.plot(kind='bar')\nplt.xlabel('Treatment Conditions')\nplt.ylabel('Percentage That Donate')\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.title('Percentage of Treatment and Control Populations that Donated')\n\n\n\n\n\n\n\n\nDid Not Give\nGave\n\n\n\n\nControl\n0.982142\n0.017858\n\n\nTreatment\n0.977961\n0.022039\n\n\n\n\n\n\n\nText(0.5, 1.0, 'Percentage of Treatment and Control Populations that Donated')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT-Test of was a donation made\n\n\n\n\n\n\n\nt_test(data, 'gave')\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n3.21\n0.02\n0.02\n0.0\n0.0\n16687\n33396\n0.001331\nsignificant\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbit Regression\n\n\n\n\n\n\n\nX = data[['treatment']]\ny = data['gave']\n\n# Fit Probit model\nprobit_model = sm.discrete.discrete_model.Probit(y, X).fit()\n\n\ndata['predicted_prob'] = probit_model.predict(X)\n# Extract key regression info\nprobit_df = pd.DataFrame({\n    \"Variable\": probit_model.params.index,\n    \"Coefficient\": probit_model.params.round(4).values,\n    \"Std. Error\": probit_model.bse.round(4).values,\n    \"z-Statistic\": probit_model.tvalues.round(2).values,\n    \"p-Value\": probit_model.pvalues.round(5).values\n})\n\nprobit_df[\"Variable\"] = probit_df[\"Variable\"].replace({'const': 'Intercept'})\nhtml_table = probit_df.to_html(index=False, classes='table table-sm table-striped', border=0)\n\n# Display result in a styled card\ncard = f\"\"\"\n&lt;div class='balance-card'&gt;\n  &lt;h4 class='balance-card h4'&gt;Probit Regression: Likelihood of Donating&lt;/h4&gt;\n  {html_table}\n  &lt;p class='balance-card p'&gt;Interpretation: The treatment variable has a statistically significant effect on the probability of donating.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\ndisplay(HTML(card))\n\n# Plot\nplt.figure(figsize=(6,4))\nmeans = data.groupby('treatment')['predicted_prob'].mean()\n\nplt.bar(['Control', 'Treatment'], means,)\nplt.ylabel(\"Predicted Probability of Donating\")\nplt.title(\"Probit Model: Predicted Probability by Treatment\")\nplt.ylim(0, means.max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\nOptimization terminated successfully.\n         Current function value: 0.301543\n         Iterations 7\n\n\n\n\n  Probit Regression: Likelihood of Donating\n  \n\n\n\nVariable\nCoefficient\nStd. Error\nz-Statistic\np-Value\n\n\n\n\ntreatment\n-2.0134\n0.0153\n-131.73\n0.0\n\n\n\n\n  Interpretation: The treatment variable has a statistically significant effect on the probability of donating.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess whether the size of the matching grant—specifically 1:1, 2:1, or 3:1—affects the likelihood that individuals make a donation. I restrict the analysis to individuals in the treatment group and conduct a series of t-tests comparing response rates across the different match ratios.\nThe results show no statistically significant differences in donation rates between the 1:1, 2:1, and 3:1 match offers. The mean differences in response rates are small, and p-values well above 0.05 confirm that these differences are not meaningful. This aligns with the authors’ observation on page 8 of the paper that “figures suggest that neither the match threshold nor the example amount had a meaningful influence on behavior.”\nIn short, the data suggest that it’s the presence of a match—not its magnitude—that motivates people to give. This finding has practical implications for fundraising: offering a basic match may be just as effective as offering a more generous one, potentially saving costs for the nonprofit without sacrificing donor engagement.\n\n\n\n\n\n\nMatch Rate Analysis\n\n\n\n\n\n\n\n# Filter to treatment group only\nmatched = data[data['treatment'] == 1]\n\n# Separate by match ratio\ngroup_1_1 = matched[matched['ratio'] == 1]['gave']\ngroup_2_1 = matched[matched['ratio'] == 2]['gave']\ngroup_3_1 = matched[matched['ratio'] == 3]['gave']\n\n# Perform t-tests\nresults = []\n\ndef run_ratio_ttest(group_a, group_b, label_a, label_b):\n    t_stat, p_val = ttest_ind(group_a, group_b, equal_var=False)\n    return {\n        'Group A': label_a,\n        'Group B': label_b,\n        'Mean A': round(group_a.mean(), 4),\n        'Mean B': round(group_b.mean(), 4),\n        'Mean Diff': round(group_b.mean() - group_a.mean(), 4),\n        't_stat': round(t_stat, 3),\n        'p_value': round(p_val, 4),\n        'significance': 'significant' if p_val &lt; 0.05 else 'not significant'\n    }\n\nresults.append(run_ratio_ttest(group_1_1, group_2_1, '1:1', '2:1'))\nresults.append(run_ratio_ttest(group_1_1, group_3_1, '1:1', '3:1'))\nresults.append(run_ratio_ttest(group_2_1, group_3_1, '2:1', '3:1'))\n\n# View as DataFrame\nresults_df = pd.DataFrame(results)\ndisplay(results_df)\n\n\n\n\n\n\n\n\nGroup A\nGroup B\nMean A\nMean B\nMean Diff\nt_stat\np_value\nsignificance\n\n\n\n\n0\n1:1\n2:1\n0.0207\n0.0226\n0.0019\n-0.965\n0.3345\nnot significant\n\n\n1\n1:1\n3:1\n0.0207\n0.0227\n0.0020\n-1.015\n0.3101\nnot significant\n\n\n2\n2:1\n3:1\n0.0226\n0.0227\n0.0001\n-0.050\n0.9600\nnot significant\n\n\n\n\n\n\n\nTo complement the t-tests, I also run a regression to assess the effect of match size on donation behavior. I restrict the sample to the treatment group and regress the binary outcome gave on indicator variables for the match ratios: 2:1 and 3:1, with 1:1 serving as the reference group. This specification allows us to estimate how each match ratio affects the likelihood of giving relative to the 1:1 baseline.\nThe regression coefficients for both 2:1 and 3:1 match ratios are small and not statistically significant. This mirrors the t-test findings and supports the paper’s conclusion that larger match ratios do not meaningfully increase participation. The statistical precision of the estimates is low, with wide confidence intervals crossing zero, reinforcing that any observed differences could plausibly be due to chance. These findings suggest that while match offers increase overall response rates, increasing the match beyond 1:1 yields no additional benefit.\n\n\n\n\n\n\nRatio Analysis\n\n\n\n\n\n\n\nmatched = data[data['treatment'] == 1].copy()\n\nmatched['ratio1'] = (matched['ratio'] == 1).astype(int)\nmatched['ratio2'] = (matched['ratio'] == 2).astype(int)\nmatched['ratio3'] = (matched['ratio'] == 3).astype(int)\nimport statsmodels.api as sm\n\nX = matched[['ratio2', 'ratio3']]  # ratio1 is the omitted (reference) group\nX = sm.add_constant(X)\ny = matched['gave']\n\nmodel = sm.OLS(y, X).fit()\n\nimport pandas as pd\nfrom IPython.display import HTML\n\n# Extract and clean key regression results\ntable = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.round(4).values,\n    \"Std. Error\": model.bse.round(4).values,\n    \"t-Statistic\": model.tvalues.round(2).values,\n    \"p-Value\": model.pvalues.round(5).values\n})\n\ntable['Variable'] = table['Variable'].replace({'const': 'Intercept'})\n\n# Convert to HTML with styling\nhtml_table = table.to_html(index=False, classes='table table-sm table-striped', border=0)\n\n# Wrap in a styled card\ncard = f\"\"\"\n&lt;div class='response-card'&gt;\n  &lt;h4 style='margin-top: 0; color: #2c3e50;'&gt;OLS Regression: Effect of Match Ratios on Giving&lt;/h4&gt;\n  {html_table}\n  &lt;p style='font-size: 0.9em; color: #555;'&gt;Note: Reference group is ratio = 1:1.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\nHTML(card)\n\n\n\n  OLS Regression: Effect of Match Ratios on Giving\n  \n\n\n\nVariable\nCoefficient\nStd. Error\nt-Statistic\np-Value\n\n\n\n\nIntercept\n0.0207\n0.0014\n14.91\n0.00000\n\n\nratio2\n0.0019\n0.0020\n0.96\n0.33828\n\n\nratio3\n0.0020\n0.0020\n1.01\n0.31332\n\n\n\n\n  Note: Reference group is ratio = 1:1.\n\n\n\nTo further examine the effectiveness of different match sizes, I calculate the difference in response rates directly from the data. The increase in giving from a 1:1 to a 2:1 match is approximately 0.19 percentage points, and from 2:1 to 3:1 is just 0.01 percentage points—both very small changes. I then compare these results to the fitted coefficients from the earlier regression, which also show similarly small and statistically insignificant differences between the match levels.\nTaken together, these results suggest that raising the match ratio does not meaningfully affect the likelihood of donating. Whether donors are offered a 1:1, 2:1, or 3:1 match, their behavior appears largely unchanged. This reinforces the paper’s central finding: the presence of a match matters, but its size does not.\n\n\n\n\n\n\nMatch Rate Regression Analysis\n\n\n\n\n\n\n\nimport pandas as pd\nfrom IPython.display import HTML\n\n# Filter for treatment group\nmatched = data[data['treatment'] == 1]\n\n# Calculate average donation rates\nmean_1_1 = matched[matched['ratio'] == 1]['gave'].mean()\nmean_2_1 = matched[matched['ratio'] == 2]['gave'].mean()\nmean_3_1 = matched[matched['ratio'] == 3]['gave'].mean()\n\n# Differences in means\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# OLS Regression: ratio as categorical\nmodel = smf.ols('gave ~ C(ratio)', data=matched).fit()\n\n# Extract coefficients\ncoef_2_1 = model.params['C(ratio)[T.2]']\ncoef_3_1 = model.params['C(ratio)[T.3]']\n\n# Difference between model coefficients\ndiff_model_2_1_vs_1_1 = coef_2_1\ndiff_model_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n# Format into HTML card\ncard = f\"\"\"\n&lt;div class='response-card'&gt;\n  &lt;h4&gt;Differences in Response Rates by Match Ratio&lt;/h4&gt;\n  &lt;ul&gt;\n    &lt;li&gt;Raw Mean Difference (2:1 vs 1:1): {diff_2_1_vs_1_1:.4f}&lt;/li&gt;\n    &lt;li&gt;Raw Mean Difference (3:1 vs 2:1): {diff_3_1_vs_2_1:.4f}&lt;/li&gt;\n    &lt;li&gt;Model Coefficient (2:1 vs 1:1): {diff_model_2_1_vs_1_1:.4f}&lt;/li&gt;\n    &lt;li&gt;Model Coefficient (3:1 vs 2:1): {diff_model_3_1_vs_2_1:.4f}&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;Note: These results suggest no statistically meaningful differences in response rate between 1:1, 2:1, and 3:1 match ratios.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\nHTML(card)\n\n\n\n  Differences in Response Rates by Match Ratio\n  \n    Raw Mean Difference (2:1 vs 1:1): 0.0019\n    Raw Mean Difference (3:1 vs 2:1): 0.0001\n    Model Coefficient (2:1 vs 1:1): -1229429281.1791\n    Model Coefficient (3:1 vs 2:1): -0.0001\n  \n  Note: These results suggest no statistically meaningful differences in response rate between 1:1, 2:1, and 3:1 match ratios.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nTo test whether the match offer affects how much donors give, I run a t-test and a simple linear regression comparing donation amounts between the treatment and control groups. This analysis includes all individuals, whether or not they donated. The results show a small increase in average donation amount in the treatment group, but the difference is only marginally statistically significant. This suggests that while match offers may slightly increase total donations on average, the effect is likely driven by more people giving—not by individuals giving more. This finding aligns with the broader conclusion that matching incentives affect participation rather than generosity.\n\n\n\n\n\n\nDonation Size Analysis\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Separate groups\ncontrol = data[data['treatment'] == 0]['amount']\ntreatment = data[data['treatment'] == 1]['amount']\n\n# T-test\nt_stat, p_value = ttest_ind(treatment, control, equal_var=False)\n\nprint(f\"t = {t_stat:.3f}, p = {p_value:.4f}\")\n\nt = 1.918, p = 0.0551\n\n\nTo isolate the effect of the treatment on the size of the donation among those who actually gave, I limit the sample to donors only and rerun the regression. This allows us to assess whether the match offer influenced how much people gave, conditional on deciding to donate. The coefficient on the treatment variable is small and statistically insignificant, indicating that among donors, those who received a match offer gave approximately the same amount as those who did not. These results suggest that the match influences whether someone donates, but not how much they give once they decide to contribute. Because the treatment was randomly assigned, the coefficient does have a causal interpretation—but here, the causal effect on donation size appears to be negligible.\n\n\n\n\n\n\nRatio Analysis\n\n\n\n\n\n\n\ndonors = data[data['amount'] &gt; 0]\nimport statsmodels.api as sm\n\nX = sm.add_constant(donors['treatment'])  # 1 if matched, 0 if not\ny = donors['amount']\n\nmodel = sm.OLS(y, X).fit()\n# Extract and clean results\nols_df = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.round(4).values,\n    \"Std. Error\": model.bse.round(4).values,\n    \"t-Statistic\": model.tvalues.round(2).values,\n    \"p-Value\": model.pvalues.round(5).values\n})\n\nols_df[\"Variable\"] = ols_df[\"Variable\"].replace({'const': 'Intercept'})\nhtml_table = ols_df.to_html(index=False, classes='table table-sm table-striped', border=0)\n\n# Styled HTML card\ncard = f\"\"\"\n&lt;div style='border: 1px solid #ccc; padding: 15px; border-radius: 10px; background-color: #f9f9f9; margin: 20px 0;'&gt;\n  &lt;h4 style='margin-top: 0; color: #2c3e50;'&gt;OLS Regression: Donation Amount (Conditional on Giving)&lt;/h4&gt;\n  {html_table}\n  &lt;p style='font-size: 0.9em; color: #555;'&gt;Interpretation: There is no statistically significant difference in donation size between treatment and control among those who chose to donate.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\nThe below graph illustrates visually that were was not a staistically signifcant delta between the contorl and treatment group in our sample.\n\n\n\n\n\n\nPlot Analysis\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to people who donated\ndonors = data[data['amount'] &gt; 0]\ncontrol_donors = donors[donors['treatment'] == 0]\ntreatment_donors = donors[donors['treatment'] == 1]\n\n# Calculate means\ncontrol_mean = control_donors['amount'].mean()\ntreatment_mean = treatment_donors['amount'].mean()\n\n# Plot side-by-side histograms\nplt.figure(figsize=(12, 5))\n\n# Control Group\nplt.subplot(1, 2, 1)\nplt.hist(control_donors['amount'], bins=30, color='green', edgecolor='black')\nplt.axvline(control_mean, color='red', linestyle='--', label=f\"Mean = {control_mean:.2f}\")\nplt.title(\"Control Group Donations\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\n# Treatment Group\nplt.subplot(1, 2, 2)\nplt.hist(treatment_donors['amount'], bins=30, color='skyblue', edgecolor='black')\nplt.axvline(treatment_mean, color='red', linestyle='--', label=f\"Mean = {treatment_mean:.2f}\")\nplt.title(\"Treatment Group Donations\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#simulation-experiment",
    "href": "projects/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\n\n\n\n\nSimulation Using Bernoullui Distrubtion\n\n\n\n\n\n\nFrom out bernoulii parameters we can caclulate the mean and the stndard deviation of the distrubtion\n\np_no =.018\nmu_no_char = p_no\nstd_no_char = (mu_no_char* (1-mu_no_char))**.5\n\np = .022\nmu_char = p\nstd_char =  (mu_char * (1-mu_char))**.5\n\nprint(f'The mean and standard deviation for the respondents who did not receive a donation match is {mu_no_char,round(std_no_char,2)}')\n\nprint(f'The mean and standard deviation for the respondents who did receive a donation match is {mu_char,round(std_char,2)}')\n\nThe mean and standard deviation for the respondents who did not receive a donation match is (0.018, 0.13)\nThe mean and standard deviation for the respondents who did receive a donation match is (0.022, 0.15)\n\n\n\nLaw of Large Numbers\nThe Law of Large numbers say the more information we have the better our estimates will be. In statics terminology as we increase our sample size the sample mean will move closer to the population mean. To see this law in action we’ll increase our sample size to 10,00 and calcuate the average as we increase the sample size. This should allign with the means that we know from our benoulli distrbution.\n\nn = 10_000\nnp.random.seed(38)\nno_match_draws = np.random.binomial(1, p_no, size=n)\nmatch_draws = np.random.binomial(1, p, size=n)\ndifferences = match_draws - no_match_draws\ncumulative_avg = np.cumsum(differences) / np.arange(1, 10001)\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg)\nplt.axhline(p - p_no, color='red', linestyle='--', label='True Difference')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Avg (Match - No Match)')\nplt.title('Law of Large Numbers in Action')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n#\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\nCentral Limit Theorem Demo\n\n\n\n\n\n\nThe central limit theorem states that as we increase the sample size of a sample the distrubtion of the samples will become more like the bell-shaped cureve, regardless of the starting distrubtion. To Test this this we’ll create 4 sample sizes from our given bernoulli distribtuion. We should expect to see the central limit theorem ‘kick-in’ and the distribtuion start to become more bellshaped.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#disibutin parameters\np_no = 0.018\np_match = 0.022\n\n#sample sizes \nsample_sizes = [50, 200, 500, 1000]\n#number of simluatoin of a sample size\nn_sim = 1000\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\n\n#loop through the different sample size\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_sim):\n        #pull a no_match from a binomial distrubtion of size 1000\n        no_match = np.random.binomial(1, p_no, size=n)\n        #pull a match from a binomial distrubtion of size 1000\n        match = np.random.binomial(1, p_match, size=n)\n        #append the differences to a running list per sample size\n        diff = match.mean() - no_match.mean()\n        diffs.append(diff)\n\n    # plot the graphs\n    axs[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(p_match - p_no, color='red', linestyle='--', label='True Difference')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Mean Difference (Match - No Match)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Histograms of Sample Mean Differences — Central Limit Theorem in Action\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Archive/index.html",
    "href": "Archive/index.html",
    "title": "HW 1",
    "section": "",
    "text": "Question 1\nProf says make a chart\n\n#|echo: false\nprint('Hello World')\n\nHello World\n\nimport matplotlib.pyplot as plt\n\nMatplotlib created a temporary cache directory at /tmp/matplotlib-gw_j1hnn because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n\nimport numpy as np\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.plot(x, y)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid()\nplt.savefig('sine_wave.png')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport sys\nprint(\"Python version:\", sys.version)\n\nPython version: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:56:51) [GCC 13.3.0]\n\n\n\n4+ 7\n\n[1] 11"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Lowell Resume",
    "section": "",
    "text": "Last updated: 2025-04-05\nDownload PDF file."
  },
  {
    "objectID": "personnel.html",
    "href": "personnel.html",
    "title": "Personnel Projects",
    "section": "",
    "text": "This is a test"
  },
  {
    "objectID": "projects/hw1/hw1_questions.html",
    "href": "projects/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was one of the largest of its kind. Notably, it was conducted for a politically motivated nonprofit using paper mail solicitations in 2005. These characteristics are important to consider, as donor behavior may have shifted in recent years. For example, individuals might become more politically active—and therefore more willing to donate—during contentious elections, while economic uncertainty could make donors more hesitant to give. Regardless of these contextual differences, we will explore how the researchers designed and analyzed their experiment, and use modern statistical techniques to replicate their findings."
  }
]