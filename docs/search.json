[
  {
    "objectID": "Personnel/Archive/hw2_notebook.html",
    "href": "Personnel/Archive/hw2_notebook.html",
    "title": "test",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nMatplotlib created a temporary cache directory at /tmp/matplotlib-c85owgi7 because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n\n\n\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\n\n\nairbnb.columns\n\nIndex(['Unnamed: 0', 'id', 'days', 'last_scraped', 'host_since', 'room_type',\n       'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n       'review_scores_cleanliness', 'review_scores_location',\n       'review_scores_value', 'instant_bookable'],\n      dtype='object')\n\n\n\nblueprinty.columns\n\nIndex(['patents', 'region', 'age', 'iscustomer'], dtype='object')\n\n\n\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nblueprint_hist = pd.pivot_table(blueprinty, index='iscustomer', values='patents', aggfunc='mean').head()\nblueprint_hist\n\n\n\n\n\n\n\n\npatents\n\n\niscustomer\n\n\n\n\n\n0\n3.473013\n\n\n1\n4.133056\n\n\n\n\n\n\n\n\nblueprint_hist = blueprint_hist.reset_index()\n\n# Create a histogram-style bar chart\nplt.figure(figsize=(6, 4))\nplt.bar(blueprint_hist['iscustomer'].astype(str), blueprint_hist['patents'], edgecolor='black')\n\n# Add labels and title\nplt.xlabel('Is Customer')\nplt.ylabel('Average Number of Patents')\nplt.title('Average Patents by Customer Status')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nblueprinty['age'].describe()\n\ncount    1500.000000\nmean       26.357667\nstd         7.242528\nmin         9.000000\n25%        21.000000\n50%        26.000000\n75%        31.625000\nmax        49.000000\nName: age, dtype: float64\n\n\n\nblueprinty['age_bins']= pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50], right=False)\nblueprint_hist_age =pd.crosstab(index=blueprinty['age_bins'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')\ndisplay(blueprint_hist_age)\n\n\nplot_data_age = blueprint_hist_age.drop(index='Total', columns='Total')\n\n# Plot\nplot_data_age.plot(\n    kind='bar',\n    stacked=False,\n    figsize=(8, 5),\n    edgecolor='black',\n    color =( 'skyblue','green',),\n)\n\n# Add labels and title\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.title('Customer Status by Age Group')\nplt.legend(title='Is Customer', labels=['No (0)', 'Yes (1)'])\n\nplt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\niscustomer\n0\n1\nTotal\n\n\nage_bins\n\n\n\n\n\n\n\n[0, 20)\n205\n98\n303\n\n\n[20, 30)\n496\n213\n709\n\n\n[30, 40)\n291\n142\n433\n\n\n[40, 50)\n27\n28\n55\n\n\nTotal\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblueprint_hist_region= pd.crosstab(index=blueprinty['region'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')\nblueprint_hist_region\n\nplot_data_region = blueprint_hist_region.drop(index='Total', columns='Total')\n\n# Plot\nplot_data_region.plot(\n    kind='bar',\n    stacked=False,\n    figsize=(8, 5),\n    edgecolor='black',\n    color=['skyblue','forestgreen']\n)\n\n# Add labels and title\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Customer Status by Region')\nplt.legend(title='Is Customer', labels=['No (0)', 'Yes (1)'])\n\nplt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom math import factorial\n\ndef poisson_distro(lmbda, y):\n    return (np.exp(-lmbda) * (lmbda ** y)) / factorial(y)\n\ndef poisson_likelihood(lmbda, y_array):\n    return np.prod([poisson_distro(lmbda, y_i) for y_i in y_array])\n\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y_array):\n    y_array = np.array(y_array)\n    return np.sum(-lmbda + y_array * np.log(lmbda) - gammaln(y_array + 1))\n\n\n\n# Evaluate log-likelihoods across lambda values\nlambda_vals = np.arange(1, 20)\nlog_likelihoods = [poisson_log_likelihood(lmbda, blueprinty['patents'].values) for lmbda in lambda_vals]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_likelihoods, marker='o', color='navy')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood_regression(beta, X, y):\n    \"\"\"\n    Log-likelihood function for Poisson regression.\n    \n    Parameters:\n    - beta: array-like, shape (k,)\n    - X: array-like, shape (n, k)  ← includes intercept if desired\n    - y: array-like, shape (n,)\n    \n    Returns:\n    - Scalar: log-likelihood value\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    beta = np.array(beta)\n    \n    beta = X @ beta                  # linear predictor\n    lambda_ = np.exp(eta)           # inverse link: ensures lambda_i &gt; 0\n    log_lik = np.sum(-lambda_ + y * beta - gammaln(y + 1))\n    \n    return log_lik\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Step 1: Simulate a simple dataset (like blueprinty)\nnp.random.seed(0)\nn = 100\nage = np.random.randint(20, 70, size=n)\nregion = np.random.choice([0, 1], size=n)  # binary region\nis_customer = np.random.choice([0, 1], size=n)\n\n# Design matrix with intercept, age, age^2, region, is_customer\nX = np.column_stack((\n    np.ones(n),           # intercept\n    age,\n    age**2,\n    region,\n    is_customer\n))\n\n# True beta (for simulation purposes)\nbeta_true = np.array([1.0, 0.02, -0.0002, 0.3, 0.5])\n\n# Simulate counts from Poisson model\neta = X @ beta_true\nlambda_ = np.exp(eta)\ny = np.random.poisson(lambda_)\n\n# Step 2: Define the Poisson regression log-likelihood\ndef poisson_log_likelihood_regression(beta, X, y):\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    return np.sum(-lambda_ + y * eta - gammaln(y + 1))\n\n# Step 3: Maximize the log-likelihood\ninitial_beta = np.zeros(X.shape[1])\n\nresult = minimize(\n    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),\n    x0=initial_beta,\n    method='BFGS'\n)\n\n# Output the estimated coefficients\nbeta_mle = result.x\nlog_lik_at_mle = -result.fun\nprint(\"Estimated Coefficients (MLE):\", beta_mle)\nprint(\"Log-Likelihood at MLE:\", log_lik_at_mle)\n\nEstimated Coefficients (MLE): [0. 0. 0. 0. 0.]\nLog-Likelihood at MLE: -1008.8436872357606\n\n\n/tmp/ipykernel_48080/4112774673.py:33: RuntimeWarning: overflow encountered in exp\n  lambda_ = np.exp(eta)\n/opt/conda/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/tmp/ipykernel_48080/4112774673.py:33: RuntimeWarning: overflow encountered in exp\n  lambda_ = np.exp(eta)\n\n\n_todo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates.\nSpecifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable.\nUse the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\ndef poisson_log_likelihood_regression(beta, X, y):\n    # Ensure all inputs are NumPy arrays\n    if not isinstance(beta, np.ndarray):\n        beta = np.asarray(beta)\n    if not isinstance(X, np.ndarray):\n        X = np.asarray(X)\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n\n    return np.sum(-lambda_ + y * eta - gammaln(y + 1))\n\n\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nblueprinty['age_scaled'] = (blueprinty['age'] - blueprinty['age'].mean()) / blueprinty['age'].std()\nblueprinty['age_squared'] = blueprinty['age_scaled'] ** 2\n\n\n\nblueprinty['intercept'] = 1\nencoded_region = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\nblueprinty = pd.concat([blueprinty, encoded_region], axis=1)\nX = blueprinty[['intercept', 'age_scaled', 'age_squared', 'region_Northeast', 'region_Northwest',\n       'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()\n\ny = blueprinty['patents'].values\ninitial_beta = np.zeros(X.shape[1])\n\n\n\nresult = minimize(\n    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),\n    x0=initial_beta,\n    method='BFGS'\n)\nbeta_mle = result.x\nlog_lik_at_mle = -result.fun\nprint(\"Estimated Coefficients (MLE):\", beta_mle)\nprint(\"Log-Likelihood at MLE:\", log_lik_at_mle)\n\nEstimated Coefficients (MLE): [ 1.34467567 -0.05772321 -0.15581386  0.02917009 -0.0175745   0.05656135\n  0.05057614  0.20759078]\nLog-Likelihood at MLE: -3258.0721454198165\n\n\n\nprint(result.success)\nprint(result.message)\n\nFalse\nDesired error not necessarily achieved due to precision loss.\n\n\n\nimport statsmodels.api as sm\n# Use the same standardized/scaled features\nX_sm = blueprinty[['age_scaled', 'age_squared', 'region_Northeast',\n                   'region_Northwest', 'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()\n\n# Add intercept (statsmodels handles it with sm.add_constant)\nX_sm = sm.add_constant(X_sm)\n\ny_sm = blueprinty['patents']\nmodel = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Sun, 04 May 2025   Deviance:                       2143.3\nTime:                        15:51:42   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.3447      0.038     35.059      0.000       1.270       1.420\nx1            -0.0577      0.015     -3.843      0.000      -0.087      -0.028\nx2            -0.1558      0.014    -11.513      0.000      -0.182      -0.129\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\n\nairbnb\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n40623\n40624\n18008937\n266\n4/2/2017\n7/10/2016\nEntire home/apt\n1.5\n2.0\n150\n0\nNaN\nNaN\nNaN\nt\n\n\n40624\n40625\n18009045\n366\n4/2/2017\n4/1/2016\nPrivate room\n1.0\n1.0\n125\n0\nNaN\nNaN\nNaN\nf\n\n\n40625\n40626\n18009065\n587\n4/2/2017\n8/24/2015\nPrivate room\n1.0\n1.0\n80\n0\nNaN\nNaN\nNaN\nt\n\n\n40626\n40627\n18009650\n335\n4/2/2017\n5/2/2016\nPrivate room\n1.0\n1.0\n69\n0\nNaN\nNaN\nNaN\nt\n\n\n40627\n40628\n18009669\n1\n4/2/2017\n4/1/2017\nEntire home/apt\n1.0\n1.0\n115\n0\nNaN\nNaN\nNaN\nt\n\n\n\n\n40628 rows × 14 columns\n\n\n\n\ncols_of_interets = ['days','bathrooms','bedrooms','number_of_reviews','review_scores_location','review_scores_value','review_scores_cleanliness',]\nfor i in cols_of_interets:\n    airbnb[i].describe()\n    plt.scatter(airbnb['price'], airbnb[i])\n    plt.xlabel('Price')\n    plt.ylabel(i)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nairbnb['price_bins'] = pd.cut(airbnb['price'], bins=[0,500,5000, 10000], right=False)\nairbnb['bathrooms_bins'] = pd.cut(airbnb['bathrooms'], bins=[0, 1, 2, 3, 4, 5, 6], right=False)\nairbnb['bedrooms_bins'] = pd.cut(airbnb['bedrooms'], bins=[0, 1, 2, 3, 4, 5, 6], right=False)\nairbnb['number_of_reviews_bins'] = pd.cut(airbnb['number_of_reviews'], bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], right=False)\n\n\nbathroom_hist = pd.pivot_table(airbnb, index='bathrooms_bins', values='number_of_reviews', aggfunc=['mean'],margins=True,margins_name='Total')\ndisplay(bathroom_hist)\nplot_data_bathrooms = bathroom_hist.drop(index='Total')\n\n# Plot\nplot_data_bathrooms.plot(\n    kind='bar',\n    stacked=False,\n    figsize=(8, 5),\n    edgecolor='black',\n    color =('green',),\n)\n\n# Add labels and title\nplt.xlabel('NR of Bathrooms')\nplt.ylabel('Average Number of Reviews')\nplt.title('Average Number of Reviews by Bathrooms')\nplt.legend().set_visible(False)\nplt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_48080/364120488.py:1: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  bathroom_hist = pd.pivot_table(airbnb, index='bathrooms_bins', values='number_of_reviews', aggfunc=['mean'],margins=True,margins_name='Total')\n\n\n\n\n\n\n\n\n\nmean\n\n\n\nnumber_of_reviews\n\n\nbathrooms_bins\n\n\n\n\n\n[0, 1)\n26.604061\n\n\n[1, 2)\n15.846360\n\n\n[2, 3)\n14.943998\n\n\n[3, 4)\n17.843854\n\n\n[4, 5)\n13.571429\n\n\n[5, 6)\n16.923077\n\n\nTotal\n15.837927\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprice_hist = pd.pivot_table(airbnb, index='price_bins', values='number_of_reviews', aggfunc=['mean'],margins=True,margins_name='Total')\ndisplay(bathroom_hist)\nplot_data_price = price_hist.drop(index='Total')\n\n# Plot\nplot_data_price.plot(\n    kind='bar',\n    stacked=False,\n    figsize=(8, 5),\n    edgecolor='black',\n    color =('green',),\n)\n\n# Add labels and title\nplt.xlabel('price_bins')\nplt.ylabel('Average Number of Reviews')\nplt.title('Average Number of Reviews by Price')\nplt.legend().set_visible(False)\nplt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_48080/3684250169.py:1: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  price_hist = pd.pivot_table(airbnb, index='price_bins', values='number_of_reviews', aggfunc=['mean'],margins=True,margins_name='Total')\n\n\n\n\n\n\n\n\n\nmean\n\n\n\nnumber_of_reviews\n\n\nbathrooms_bins\n\n\n\n\n\n[0, 1)\n26.604061\n\n\n[1, 2)\n15.846360\n\n\n[2, 3)\n14.943998\n\n\n[3, 4)\n17.843854\n\n\n[4, 5)\n13.571429\n\n\n[5, 6)\n16.923077\n\n\nTotal\n15.837927\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfrom statsmodels.api import GLM\nfrom statsmodels.genmod.families import Poisson\n\n# Step 1: Exploratory Data Analysis (EDA)\n\n# Distribution of the number of reviews\nsns.histplot(airbnb['number_of_reviews'], bins=30, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.show()\n\n# Correlation heatmap for numerical variables\nnumerical_cols = ['days', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews']\nsns.heatmap(airbnb[numerical_cols].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show()\n\n# Scatter plot of price vs. number of reviews\nsns.scatterplot(x='price', y='number_of_reviews', data=airbnb)\nplt.title('Price vs. Number of Reviews')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n# Step 2: Handle Missing Values\n# Drop rows with missing values in relevant columns\nrelevant_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms']\nairbnb_cleaned = airbnb.dropna(subset=relevant_cols)\n\n# Step 3: Build a Poisson Regression Model\n\n# Define dependent and independent variables\nX = airbnb_cleaned[['price', 'bathrooms', 'bedrooms']]\nX = sm.add_constant(X)  # Add intercept\ny = airbnb_cleaned['number_of_reviews']\n\n# Fit the Poisson regression model\npoisson_model = GLM(y, X, family=Poisson()).fit()\n\n# Step 4: Interpret Model Coefficients\nprint(poisson_model.summary())\n\n# Interpretation:\n# The coefficients represent the log change in the expected number of reviews for a one-unit increase in the predictor.\n# For example, if the coefficient for `price` is -0.01, it means a one-unit increase in price decreases the expected number of reviews by approximately 1% (exp(-0.01) ≈ 0.99).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                40395\nModel:                            GLM   Df Residuals:                    40391\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -7.2357e+05\nDate:                Sun, 04 May 2025   Deviance:                   1.3240e+06\nTime:                        15:54:33   Pearson chi2:                 2.16e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.09222\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.8149      0.004    694.031      0.000       2.807       2.823\nprice         -0.0002    9.7e-06    -25.475      0.000      -0.000      -0.000\nbathrooms     -0.1340      0.004    -35.012      0.000      -0.141      -0.126\nbedrooms       0.1139      0.002     56.587      0.000       0.110       0.118\n==============================================================================\n\n\n\nairbnb = airbnb[airbnb['price'] &lt;= 500]\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.api import GLM\nfrom statsmodels.genmod.families import Poisson\n\n# Step 1: Exploratory Data Analysis (EDA)\n\n# Drop rows with missing values across all relevant columns (before plotting)\neda_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms', 'days']\nairbnb_eda = airbnb.dropna(subset=eda_cols)\n\n# Distribution of the number of reviews\nsns.histplot(airbnb_eda['number_of_reviews'], bins=30, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.show()\n\n# Correlation heatmap for numerical variables\nnumerical_cols = ['days', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews']\nsns.heatmap(airbnb_eda[numerical_cols].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show()\n\n# Scatter plot of price vs. number of reviews\nsns.scatterplot(x='price', y='number_of_reviews', data=airbnb_eda)\nplt.title('Price vs. Number of Reviews')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n# Step 2: Clean data by dropping rows with missing values in model inputs\nrelevant_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms']\nairbnb_cleaned = airbnb.dropna(subset=relevant_cols)\n\n# Step 3: Build a Poisson Regression Model\n\n# Define dependent and independent variables\nX = airbnb_cleaned[['price', 'bathrooms', 'bedrooms']]\nX = sm.add_constant(X)  # Add intercept\ny = airbnb_cleaned['number_of_reviews']\n\n# Fit the Poisson regression model\npoisson_model = GLM(y, X, family=Poisson()).fit()\n\n# Step 4: Interpret Model Coefficients\nprint(poisson_model.summary())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                39654\nModel:                            GLM   Df Residuals:                    39650\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -7.1266e+05\nDate:                Sun, 04 May 2025   Deviance:                   1.3039e+06\nTime:                        16:00:24   Pearson chi2:                 2.13e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.07089\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.7975      0.005    620.600      0.000       2.789       2.806\nprice          0.0002   1.58e-05     10.273      0.000       0.000       0.000\nbathrooms     -0.1415      0.004    -34.929      0.000      -0.149      -0.134\nbedrooms       0.0928      0.002     43.494      0.000       0.089       0.097\n=============================================================================="
  },
  {
    "objectID": "personnel.html",
    "href": "personnel.html",
    "title": "Personnel Projects",
    "section": "",
    "text": "This is a test"
  },
  {
    "objectID": "projects/hw3/hw3_questions.html",
    "href": "projects/hw3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/hw3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "projects/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv('conjoint_data.csv')\ndata.head()\n\n   resp  task  choice brand   ad  price\n0     1     1       1     N  Yes     28\n1     1     1       0     H  Yes     16\n2     1     1       0     P  Yes     16\n3     1     2       0     N  Yes     32\n4     1     2       1     P  Yes     16\n\ndata = pd.get_dummies(data,columns=['brand','ad'],drop_first=True)\ndata.head()\n\n   resp  task  choice  price  brand_N  brand_P  ad_Yes\n0     1     1       1     28     True    False    True\n1     1     1       0     16    False    False    True\n2     1     1       0     16    False     True    True\n3     1     2       0     32     True    False    True\n4     1     2       1     16    False     True    True"
  },
  {
    "objectID": "projects/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\n\n\ndef log_likelihood(beta, X, y, group_indicator):\n    utilities = np.dot(X, beta)         # ensure matrix multiplication\n    utilities = np.asarray(utilities, dtype=np.float64)  # enforce numeric array\n    exp_utilities = np.exp(utilities)   # now safe\n\n    task_sums = group_indicator.T @ exp_utilities\n    denom_per_row = group_indicator @ task_sums\n    probs = exp_utilities / (denom_per_row + 1e-10)\n    log_probs = np.log(probs + 1e-10)\n\n    return -np.sum(y * log_probs)\n\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval.\n\nimport numpy as np\n\nX = data[['brand_N', 'brand_P', 'ad_Yes', 'price']].values\ny =data['choice'].values\n\n# Step 1: Get unique task identifiers (e.g., (resp, task) pairs)\ngroups = data[['resp', 'task']].drop_duplicates().reset_index(drop=True)\n\n# Step 2: Build a zero matrix: rows = alternatives, columns = tasks\nn_rows = data.shape[0]\nn_tasks = groups.shape[0]\ngroup_indicator = np.zeros((n_rows, n_tasks))\n\n# Step 3: Fill in 1s where each row belongs to a specific task\nfor task_index, (resp_id, task_id) in groups.iterrows():\n    mask = (data['resp'] == resp_id) & (data['task'] == task_id)\n    group_indicator[mask.values, task_index] = 1\n\n\nbeta_init = np.zeros(X.shape[1])  # often start with all 0s\n\nfrom scipy.optimize import minimize\n\nresult = minimize(\n    fun=log_likelihood,\n    x0=beta_init,\n    args=(X, y, group_indicator),\n    method='BFGS'\n)\n\n\nbeta_mle = result.x  # MLE estimates\nhessian_inv = result.hess_inv  # estimated inverse Hessian\nstandard_errors = np.sqrt(np.diag(hessian_inv))  # approximate SEs\n\n\nimport pandas as pd\nimport numpy as np\n\n# 95% confidence intervals\nconf_ints = np.column_stack([\n    beta_mle - 1.96 * standard_errors,\n    beta_mle + 1.96 * standard_errors\n])\n\n# Feature names (ensure they match the columns in X)\nfeature_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']\n\n# Create results DataFrame\nresults_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Std. Error': standard_errors,\n    'CI Lower (95%)': conf_ints[:, 0],\n    'CI Upper (95%)': conf_ints[:, 1]\n}, index=feature_names)\n\n# Display table\nprint(results_df)\n\n         Coefficient  Std. Error  CI Lower (95%)  CI Upper (95%)\nbrand_N     0.941195    0.113103        0.719514        1.162876\nbrand_P     0.501616    0.121766        0.262953        0.740278\nad_Yes     -0.731994    0.087630       -0.903748       -0.560240\nprice      -0.099480    0.006301       -0.111831       -0.087130"
  },
  {
    "objectID": "projects/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nMatplotlib created a temporary cache directory at /tmp/matplotlib-h2x17enf because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n\n# Ensure numeric stability and float64 types\nX = X.astype(np.float64)\ny = y.astype(np.float64)\ngroup_indicator = group_indicator.astype(np.float64)\n\n# Modified log-posterior function: log-likelihood + log-prior\ndef log_posterior(beta, X, y, group_indicator):\n    # log-likelihood\n    utilities = np.dot(X, beta)\n    exp_utilities = np.exp(utilities)\n    task_sums = group_indicator.T @ exp_utilities\n    denom_per_row = group_indicator @ task_sums\n    probs = exp_utilities / (denom_per_row + 1e-10)\n    log_probs = np.log(probs + 1e-10)\n    log_lik = np.sum(y * log_probs)\n\n    # log-prior\n    # priors: N(0, 5) for first 3 (binary vars), N(0, 1) for price\n    log_prior = (\n        -0.5 * (beta[0]**2 / 5)\n        -0.5 * (beta[1]**2 / 5)\n        -0.5 * (beta[2]**2 / 5)\n        -0.5 * (beta[3]**2 / 1)\n    )\n\n    return log_lik + log_prior\n\n# MCMC settings\nn_iter = 11000\nn_burn = 1000\nn_params = X.shape[1]\ncurrent_beta = np.zeros(n_params)\nsamples = np.zeros((n_iter, n_params))\ncurrent_log_post = log_posterior(current_beta, X, y, group_indicator)\n\n# Proposal standard deviations\nproposal_sds = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Metropolis-Hastings sampler\nfor i in range(n_iter):\n    proposal = current_beta + np.random.normal(0, proposal_sds)\n    proposal_log_post = log_posterior(proposal, X, y, group_indicator)\n\n    # Accept/reject\n    accept_prob = np.exp(proposal_log_post - current_log_post)\n    if np.random.rand() &lt; accept_prob:\n        current_beta = proposal\n        current_log_post = proposal_log_post\n\n    samples[i] = current_beta\n\n# Discard burn-in\nsamples_post_burn = samples[n_burn:]\n\n# Posterior summaries\nposterior_means = samples_post_burn.mean(axis=0)\nposterior_sds = samples_post_burn.std(axis=0)\nposterior_cis = np.percentile(samples_post_burn, [2.5, 97.5], axis=0).T\n\n# Results DataFrame\nparam_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nposterior_summary = pd.DataFrame({\n    'Posterior Mean': posterior_means,\n    'Posterior Std': posterior_sds,\n    '95% CI Lower': posterior_cis[:, 0],\n    '95% CI Upper': posterior_cis[:, 1]\n}, index=param_names)\n\n\nprint(posterior_summary)\n\n         Posterior Mean  Posterior Std  95% CI Lower  95% CI Upper\nbrand_N        0.933656       0.113566      0.719171      1.171390\nbrand_P        0.491293       0.114663      0.281182      0.724208\nad_Yes        -0.730734       0.086280     -0.905841     -0.562095\nprice         -0.099849       0.006241     -0.112272     -0.087599\n\n# Plot for price parameter (index 3)\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(samples_post_burn[:, 3])\nplt.title('Trace Plot for β_price')\nplt.xlabel('Iteration')\nplt.ylabel('β_price')\n\nplt.subplot(1, 2, 2)\nplt.hist(samples_post_burn[:, 3], bins=30, edgecolor='k')\nplt.title('Posterior Histogram for β_price')\nplt.xlabel('β_price')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/hw3/hw3_questions.html#discussion",
    "href": "projects/hw3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.\nSuppose you did not simulate the data. What do you observe about the parameter estimates? What does it mean if the Netflix coefficient is larger than the Prime coefficient? Does it make sense that the price coefficient is negative?\nIf the data were not simulated, the parameter estimates would be interpreted directly based on their signs, magnitudes, and significance. For example, a positive coefficient for the brand_N variable would suggest that respondents prefer Netflix over the baseline brand (such as Hulu). If the coefficient for Netflix is greater than the coefficient for Amazon Prime, it indicates that, on average, respondents derive more utility from choosing Netflix than Prime, all else being equal.\nA negative coefficient for price makes intuitive sense — it reflects that as the price of an option increases, the likelihood of it being chosen decreases. This is consistent with standard economic theory and suggests that the model is capturing realistic price sensitivity.\nAt a high level, what would need to change in order to simulate and estimate a multi-level (random-parameter or hierarchical) model?\nIn a basic model, we assume that all respondents share the same set of preferences (i.e., the same coefficients). In a hierarchical model, we instead allow each respondent to have their own set of coefficients. These individual-level coefficients are drawn from a population-level distribution with a mean and variance that we estimate.\nTo simulate such data, you would:\nDraw individual preference vectors (coefficients) for each respondent from a normal distribution centered around a population mean, with some standard deviation to introduce heterogeneity. Use those individual-level preferences to generate choices for each task. To estimate this type of model, you would:\nEstimate both the population-level parameters (mean and variance of preferences) and the individual-level parameters (one set of coefficients per respondent). Use methods like Hierarchical Bayes (HB) or mixed logit models that rely on simulation or MCMC to estimate the distribution of preferences. This approach provides richer insights into how preferences vary across individuals and is commonly used in real-world conjoint studies to enable better targeting and segmentation."
  },
  {
    "objectID": "Archive/index.html",
    "href": "Archive/index.html",
    "title": "HW 1",
    "section": "",
    "text": "Question 1\nProf says make a chart\n\n#|echo: false\nprint('Hello World')\n\nHello World\n\nimport matplotlib.pyplot as plt\n\nMatplotlib created a temporary cache directory at /tmp/matplotlib-zllxnd2_ because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n\nimport numpy as np\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.plot(x, y)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid()\nplt.savefig('sine_wave.png')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport sys\nprint(\"Python version:\", sys.version)\n\nPython version: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:56:51) [GCC 13.3.0]\n\n\n\n4+ 7\n\n[1] 11"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Lowell Resume",
    "section": "",
    "text": "Last updated: 2025-04-05\nDownload PDF file."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html",
    "href": "projects/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\n\n\nblueprint_hist = pd.pivot_table(blueprinty, index='iscustomer', values='patents', aggfunc='mean').head()\ndisplay(blueprint_hist)\n\nblueprint_hist = blueprint_hist.reset_index()\n\n\n\n\n\n\n\n\npatents\n\n\niscustomer\n\n\n\n\n\n0\n3.473013\n\n\n1\n4.133056\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average if a firm is a customer they have slightly more patents then a noncustomer\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty['age_bins']= pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50], right=False)\nblueprint_hist_age =pd.crosstab(index=blueprinty['age_bins'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')\ndisplay(blueprint_hist_age)\n\n\n\n\n\n\n\niscustomer\n0\n1\nTotal\n\n\nage_bins\n\n\n\n\n\n\n\n[0, 20)\n205\n98\n303\n\n\n[20, 30)\n496\n213\n709\n\n\n[30, 40)\n291\n142\n433\n\n\n[40, 50)\n27\n28\n55\n\n\nTotal\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblueprint_hist_region= pd.crosstab(index=blueprinty['region'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')\ndisplay(blueprint_hist_region)\n\n\n\n\n\n\n\niscustomer\n0\n1\nTotal\n\n\nregion\n\n\n\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nTotal\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost Firms are in the 20 - 30 age range. Additoinaly there is a disporpoiante amount of customers who are in the the North East region. We may see a higher coeffcient weight on these 2 characteristcs due to them being overrepresented in the data.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\n\n\nLog Likehood for Poission\n\n\n\n\n\n\n\n\n\n\n\nThe probability of observing a count \\(( y_i )\\) for observation \\(( i )\\), given Poisson rate  \\(( \\lambda_i )\\), is:\n\\[\nP(Y_i = y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n\\]\n\n\n\nAssuming we observe \\(( n )\\) independent data points \\(( y_1, y_2, \\ldots, y_n )\\), each with rate \\(( \\lambda_i )\\), the joint likelihood is the product of the individual probabilities: \\[\n\\mathcal{L}(\\lambda_1, \\ldots, \\lambda_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n\\]\n\n\n\nTaking the natural logarithm of the likelihood simplifies the product into a sum: \\[\n\\log \\mathcal{L} = \\sum_{i=1}^{n} \\left( -\\lambda_i + y_i \\log(\\lambda_i) - \\log(y_i!) \\right)\n\\] This is the log-likelihood function for the Poisson model.\n\n\n\n\n\n\nLog likelihood in code\n\n\n\n\n\n\n\nimport numpy as np\nfrom math import factorial\n\ndef poisson_distro(lmbda, y):\n    return (np.exp(-lmbda) * (lmbda ** y)) / factorial(y)\n\ndef poisson_likelihood(lmbda, y_array):\n    return np.prod([poisson_distro(lmbda, y_i) for y_i in y_array])\n\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y_array):\n    y_array = np.array(y_array)\n    return np.sum(-lmbda + y_array * np.log(lmbda) - gammaln(y_array + 1))\n\nWe changed from the factorial to the gammln function in the log likelihood function. When using factrioal numbers get extremely large and could cause overflow errors. Gammaln works like an integral of the factorial function which will make the math less computatiaonlly expensivve\n\n# Evaluate log-likelihoods across lambda values\nlambda_vals = np.arange(1, 20)\nlog_likelihoods = [poisson_log_likelihood(lmbda, blueprinty['patents'].values) for lmbda in lambda_vals]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe log-likelihood for the entire sample is:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + y_i \\log(\\lambda) - \\log(y_i!) \\right)\n\\]\nSimplify:\n\\[\n\\ell(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n y_i \\right) \\log(\\lambda) - \\sum_{i=1}^n \\log(y_i!)\n\\]\n\n\n\n\nTo find the MLE, take the derivative and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\ell(\\lambda) = -n + \\frac{\\sum y_i}{\\lambda}\n\\]\nSet the derivative to zero:\n\\[\n-n + \\frac{\\sum y_i}${\\lambda} = 0\n\\]\n\n\n\n\n\\[\n\\frac{\\sum y_i}{\\lambda} = n\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{n} \\sum y_i\n= \\bar{y}\n\\]\nSo the MLE for \\(( \\lambda )\\) in a Poisson distribution is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{y}\n\\]\n\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n# ecause scipy.optimize minimizes by default, we minimize the negative log-likelihood\n\n#Trick the minimze function into finding the maximum log likelhood with a negative\n   #lambda lmbda --&gt; Keep calling the function with lambda as the lmbda value\nobjective = lambda lmbda: -poisson_log_likelihood(lmbda, blueprinty['patents'].values)\n\n# Perform the optimization using bounded scalar minimization\nresult = minimize_scalar(objective, bounds=(0.01, 20), method='bounded')\n\n# Output the MLE estimate for lambda\nlambda_mle = result.x\nlog_likelihood_at_mle = -result.fun\n\n\n\n\n\n   MLE for λ: 3.6847\n   sample mean : 3.6847\n   Log-Likelihood at MLE: -3367.68\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe assume the Poisson rate parameter \\(( \\lambda )\\) varies by observation based on covariates \\(( X_i )\\) and a parameter vector \\(( \\beta )\\):\n\\[\n\\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures \\(( \\lambda_i &gt; 0 )\\) for all \\(( i )\\), as required for Poisson distributions. The exponential function is the canonical inverse link function for Poisson regression.\n\n\n\n\nGiven the model \\(( Y_i \\sim \\text{Poisson}(\\lambda_i) )\\), the log-likelihood function across all observations is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ -\\exp(X_i^\\top \\beta) + y_i (X_i^\\top \\beta) - \\log(y_i!) \\right]\n\\]\nThis is the function we will maximize to estimate \\(( \\beta )\\) using maximum likelihood estimation.\n\ndef poisson_log_likelihood_regression(beta, X, y):\n    # Ensure all inputs are NumPy arrays\n    if not isinstance(beta, np.ndarray):\n        beta = np.asarray(beta)\n    if not isinstance(X, np.ndarray):\n        X = np.asarray(X)\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n\n    return np.sum(-lambda_ + y * eta - gammaln(y + 1))\n\n\nfrom scipy.optimize import minimize\nblueprinty['age_scaled'] = (blueprinty['age'] - blueprinty['age'].mean()) / blueprinty['age'].std()\nblueprinty['age_squared'] = blueprinty['age_scaled'] ** 2\n\n\n\nblueprinty['intercept'] = 1\nencoded_region = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\nblueprinty = pd.concat([blueprinty, encoded_region], axis=1)\n\nX = blueprinty[['intercept', 'age_scaled', 'age_squared', 'region_Northeast', 'region_Northwest',\n       'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()\n\ny = blueprinty['patents'].values\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(\n    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),\n    x0=initial_beta,\n    method='BFGS'\n)\nbeta_mle = result.x\nlog_lik_at_mle = -result.fun\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n\n\n\n\n\nVariable\nEstimate (MLE)\nStd. Error\n\n\n\n\nintercept\n1.3447\n0.0366\n\n\nage_scaled\n-0.0577\n0.0151\n\n\nage_squared\n-0.1558\n0.0131\n\n\nregion_Northeast\n0.0292\n0.0456\n\n\nregion_Northwest\n-0.0176\n0.0553\n\n\nregion_South\n0.0566\n0.0543\n\n\nregion_Southwest\n0.0506\n0.0505\n\n\niscustomer\n0.2076\n0.0312\n\n\n\n\n\n\n\n  Log-Likelihood at MLE: -3258.07\n\n\n\n\nimport statsmodels.api as sm\n# Use the same standardized/scaled features\nX_sm = blueprinty[['age_scaled', 'age_squared', 'region_Northeast',\n                   'region_Northwest', 'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()\n\n# Add intercept (statsmodels handles it with sm.add_constant)\nX_sm = sm.add_constant(X_sm)\n\ny_sm = blueprinty['patents']\nmodel = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Sat, 24 May 2025   Deviance:                       2143.3\nTime:                        16:12:40   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.3447      0.038     35.059      0.000       1.270       1.420\nx1            -0.0577      0.015     -3.843      0.000      -0.087      -0.028\nx2            -0.1558      0.014    -11.513      0.000      -0.182      -0.129\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe strongest effect is from iscustomer Age has a mild downward effect Region effects are minor\n\n# Step 1: Copy X and set iscustomer to 0 and 1 for every firm\nX_0 = X.copy()\nX_1 = X.copy()\n\niscustomer_index = column_names.index(\"iscustomer\")  # Adjust if you know the column index directly\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\n\n# Step 2: Predict lambda (expected # patents) for both scenarios\ny_pred_0 = np.exp(X_0 @ beta_mle)\ny_pred_1 = np.exp(X_1 @ beta_mle)\n\n# Step 3: Estimate average treatment effect of being a customer\navg_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\nEstimated average effect of using Blueprinty's software on patent success: 0.7928"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\n\n\nblueprint_hist = pd.pivot_table(blueprinty, index='iscustomer', values='patents', aggfunc='mean').head()\ndisplay(blueprint_hist)\n\nblueprint_hist = blueprint_hist.reset_index()\n\n\n\n\n\n\n\n\npatents\n\n\niscustomer\n\n\n\n\n\n0\n3.473013\n\n\n1\n4.133056\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average if a firm is a customer they have slightly more patents then a noncustomer\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty['age_bins']= pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50], right=False)\nblueprint_hist_age =pd.crosstab(index=blueprinty['age_bins'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')\ndisplay(blueprint_hist_age)\n\n\n\n\n\n\n\niscustomer\n0\n1\nTotal\n\n\nage_bins\n\n\n\n\n\n\n\n[0, 20)\n205\n98\n303\n\n\n[20, 30)\n496\n213\n709\n\n\n[30, 40)\n291\n142\n433\n\n\n[40, 50)\n27\n28\n55\n\n\nTotal\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblueprint_hist_region= pd.crosstab(index=blueprinty['region'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')\ndisplay(blueprint_hist_region)\n\n\n\n\n\n\n\niscustomer\n0\n1\nTotal\n\n\nregion\n\n\n\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nTotal\n1019\n481\n1500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost Firms are in the 20 - 30 age range. Additoinaly there is a disporpoiante amount of customers who are in the the North East region. We may see a higher coeffcient weight on these 2 characteristcs due to them being overrepresented in the data.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\n\n\nLog Likehood for Poission\n\n\n\n\n\n\n\n\n\n\n\nThe probability of observing a count \\(( y_i )\\) for observation \\(( i )\\), given Poisson rate  \\(( \\lambda_i )\\), is:\n\\[\nP(Y_i = y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n\\]\n\n\n\nAssuming we observe \\(( n )\\) independent data points \\(( y_1, y_2, \\ldots, y_n )\\), each with rate \\(( \\lambda_i )\\), the joint likelihood is the product of the individual probabilities: \\[\n\\mathcal{L}(\\lambda_1, \\ldots, \\lambda_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n\\]\n\n\n\nTaking the natural logarithm of the likelihood simplifies the product into a sum: \\[\n\\log \\mathcal{L} = \\sum_{i=1}^{n} \\left( -\\lambda_i + y_i \\log(\\lambda_i) - \\log(y_i!) \\right)\n\\] This is the log-likelihood function for the Poisson model.\n\n\n\n\n\n\nLog likelihood in code\n\n\n\n\n\n\n\nimport numpy as np\nfrom math import factorial\n\ndef poisson_distro(lmbda, y):\n    return (np.exp(-lmbda) * (lmbda ** y)) / factorial(y)\n\ndef poisson_likelihood(lmbda, y_array):\n    return np.prod([poisson_distro(lmbda, y_i) for y_i in y_array])\n\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y_array):\n    y_array = np.array(y_array)\n    return np.sum(-lmbda + y_array * np.log(lmbda) - gammaln(y_array + 1))\n\nWe changed from the factorial to the gammln function in the log likelihood function. When using factrioal numbers get extremely large and could cause overflow errors. Gammaln works like an integral of the factorial function which will make the math less computatiaonlly expensivve\n\n# Evaluate log-likelihoods across lambda values\nlambda_vals = np.arange(1, 20)\nlog_likelihoods = [poisson_log_likelihood(lmbda, blueprinty['patents'].values) for lmbda in lambda_vals]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe log-likelihood for the entire sample is:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + y_i \\log(\\lambda) - \\log(y_i!) \\right)\n\\]\nSimplify:\n\\[\n\\ell(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n y_i \\right) \\log(\\lambda) - \\sum_{i=1}^n \\log(y_i!)\n\\]\n\n\n\n\nTo find the MLE, take the derivative and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\ell(\\lambda) = -n + \\frac{\\sum y_i}{\\lambda}\n\\]\nSet the derivative to zero:\n\\[\n-n + \\frac{\\sum y_i}${\\lambda} = 0\n\\]\n\n\n\n\n\\[\n\\frac{\\sum y_i}{\\lambda} = n\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{n} \\sum y_i\n= \\bar{y}\n\\]\nSo the MLE for \\(( \\lambda )\\) in a Poisson distribution is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{y}\n\\]\n\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n# ecause scipy.optimize minimizes by default, we minimize the negative log-likelihood\n\n#Trick the minimze function into finding the maximum log likelhood with a negative\n   #lambda lmbda --&gt; Keep calling the function with lambda as the lmbda value\nobjective = lambda lmbda: -poisson_log_likelihood(lmbda, blueprinty['patents'].values)\n\n# Perform the optimization using bounded scalar minimization\nresult = minimize_scalar(objective, bounds=(0.01, 20), method='bounded')\n\n# Output the MLE estimate for lambda\nlambda_mle = result.x\nlog_likelihood_at_mle = -result.fun\n\n\n\n\n\n   MLE for λ: 3.6847\n   sample mean : 3.6847\n   Log-Likelihood at MLE: -3367.68\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe assume the Poisson rate parameter \\(( \\lambda )\\) varies by observation based on covariates \\(( X_i )\\) and a parameter vector \\(( \\beta )\\):\n\\[\n\\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures \\(( \\lambda_i &gt; 0 )\\) for all \\(( i )\\), as required for Poisson distributions. The exponential function is the canonical inverse link function for Poisson regression.\n\n\n\n\nGiven the model \\(( Y_i \\sim \\text{Poisson}(\\lambda_i) )\\), the log-likelihood function across all observations is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ -\\exp(X_i^\\top \\beta) + y_i (X_i^\\top \\beta) - \\log(y_i!) \\right]\n\\]\nThis is the function we will maximize to estimate \\(( \\beta )\\) using maximum likelihood estimation.\n\ndef poisson_log_likelihood_regression(beta, X, y):\n    # Ensure all inputs are NumPy arrays\n    if not isinstance(beta, np.ndarray):\n        beta = np.asarray(beta)\n    if not isinstance(X, np.ndarray):\n        X = np.asarray(X)\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n\n    return np.sum(-lambda_ + y * eta - gammaln(y + 1))\n\n\nfrom scipy.optimize import minimize\nblueprinty['age_scaled'] = (blueprinty['age'] - blueprinty['age'].mean()) / blueprinty['age'].std()\nblueprinty['age_squared'] = blueprinty['age_scaled'] ** 2\n\n\n\nblueprinty['intercept'] = 1\nencoded_region = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\nblueprinty = pd.concat([blueprinty, encoded_region], axis=1)\n\nX = blueprinty[['intercept', 'age_scaled', 'age_squared', 'region_Northeast', 'region_Northwest',\n       'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()\n\ny = blueprinty['patents'].values\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(\n    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),\n    x0=initial_beta,\n    method='BFGS'\n)\nbeta_mle = result.x\nlog_lik_at_mle = -result.fun\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n\n\n\n\n\nVariable\nEstimate (MLE)\nStd. Error\n\n\n\n\nintercept\n1.3447\n0.0366\n\n\nage_scaled\n-0.0577\n0.0151\n\n\nage_squared\n-0.1558\n0.0131\n\n\nregion_Northeast\n0.0292\n0.0456\n\n\nregion_Northwest\n-0.0176\n0.0553\n\n\nregion_South\n0.0566\n0.0543\n\n\nregion_Southwest\n0.0506\n0.0505\n\n\niscustomer\n0.2076\n0.0312\n\n\n\n\n\n\n\n  Log-Likelihood at MLE: -3258.07\n\n\n\n\nimport statsmodels.api as sm\n# Use the same standardized/scaled features\nX_sm = blueprinty[['age_scaled', 'age_squared', 'region_Northeast',\n                   'region_Northwest', 'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()\n\n# Add intercept (statsmodels handles it with sm.add_constant)\nX_sm = sm.add_constant(X_sm)\n\ny_sm = blueprinty['patents']\nmodel = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nprint(result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Sat, 24 May 2025   Deviance:                       2143.3\nTime:                        16:12:40   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.3447      0.038     35.059      0.000       1.270       1.420\nx1            -0.0577      0.015     -3.843      0.000      -0.087      -0.028\nx2            -0.1558      0.014    -11.513      0.000      -0.182      -0.129\nx3             0.0292      0.044      0.669      0.504      -0.056       0.115\nx4            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx5             0.0566      0.053      1.074      0.283      -0.047       0.160\nx6             0.0506      0.047      1.072      0.284      -0.042       0.143\nx7             0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe strongest effect is from iscustomer Age has a mild downward effect Region effects are minor\n\n# Step 1: Copy X and set iscustomer to 0 and 1 for every firm\nX_0 = X.copy()\nX_1 = X.copy()\n\niscustomer_index = column_names.index(\"iscustomer\")  # Adjust if you know the column index directly\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\n\n# Step 2: Predict lambda (expected # patents) for both scenarios\ny_pred_0 = np.exp(X_0 @ beta_mle)\ny_pred_1 = np.exp(X_1 @ beta_mle)\n\n# Step 3: Estimate average treatment effect of being a customer\navg_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\nEstimated average effect of using Blueprinty's software on patent success: 0.7928"
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#airbnb-case-study",
    "href": "projects/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\nairbnb['price_bins'] = pd.cut(airbnb['price'], bins=[0,500,5000, 10000], right=False)\nairbnb['bathrooms_bins'] = pd.cut(airbnb['bathrooms'], bins=[0, 1, 2, 3, 4, 5, 6], right=False)\nairbnb['bedrooms_bins'] = pd.cut(airbnb['bedrooms'], bins=[0, 1, 2, 3, 4, 5, 6], right=False)\nairbnb['number_of_reviews_bins'] = pd.cut(airbnb['number_of_reviews'], bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], right=False)\n\n\n\n\n\n\n\n\n\n\nmean\n\n\n\nnumber_of_reviews\n\n\nbathrooms_bins\n\n\n\n\n\n[0, 1)\n26.604061\n\n\n[1, 2)\n15.846360\n\n\n[2, 3)\n14.943998\n\n\n[3, 4)\n17.843854\n\n\n[4, 5)\n13.571429\n\n\n[5, 6)\n16.923077\n\n\nTotal\n15.837927\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\n\n\n\nnumber_of_reviews\n\n\nbathrooms_bins\n\n\n\n\n\n[0, 1)\n26.604061\n\n\n[1, 2)\n15.846360\n\n\n[2, 3)\n14.943998\n\n\n[3, 4)\n17.843854\n\n\n[4, 5)\n13.571429\n\n\n[5, 6)\n16.923077\n\n\nTotal\n15.837927\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nairbnb = airbnb[airbnb['price'] &lt;= 500]\n\nDrop rows with missing values across all relevant columns (before plotting)\n\neda_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms', 'days']\nairbnb_eda = airbnb.dropna(subset=eda_cols)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                39654\nModel:                            GLM   Df Residuals:                    39650\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -7.1266e+05\nDate:                Sat, 24 May 2025   Deviance:                   1.3039e+06\nTime:                        16:12:41   Pearson chi2:                 2.13e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.07089\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.7975      0.005    620.600      0.000       2.789       2.806\nprice          0.0002   1.58e-05     10.273      0.000       0.000       0.000\nbathrooms     -0.1415      0.004    -34.929      0.000      -0.149      -0.134\nbedrooms       0.0928      0.002     43.494      0.000       0.089       0.097\n==============================================================================\n\n\n\nairbnb_eda['intercept'] = 1\nX = airbnb_eda[['intercept','price', 'bathrooms', 'bedrooms']].astype(float).to_numpy()\ny = airbnb_eda['number_of_reviews'].values\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(\n    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),\n    x0=initial_beta,\n    method='BFGS'\n)\nbeta_mle = result.x\nlog_lik_at_mle = -result.fun\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\ncoef_df = pd.DataFrame({\n    'Variable': ['intercept','price', 'bathrooms', 'bedrooms'],\n    'Estimate (MLE)': [f\"{val:.4f}\" for val in beta_mle],\n    'Std. Error': [f\"{se:.4f}\" for se in standard_errors]\n})\n\n\n\n\n\n\nVariable\nEstimate (MLE)\nStd. Error\n\n\n\n\nintercept\n2.7975\n0.0047\n\n\nprice\n0.0002\n0.0000\n\n\nbathrooms\n-0.1415\n0.0043\n\n\nbedrooms\n0.0928\n0.0021\n\n\n\n\n\n\n\n  Log-Likelihood at MLE: -712663.27\n\n\n\nIn this Poisson regression model, each additional bedroom is associated with a 9.3% increase in expected reviews. Bathrooms have a negative effect, with each additional bathroom linked to a 14% decrease. Price has a statistically significant but negligible impact, increasing expected reviews by just 0.02% per dollar."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html",
    "href": "projects/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was one of the largest of its kind. Notably, it was conducted for a politically motivated nonprofit using paper mail solicitations in 2005. These characteristics are important to consider, as donor behavior may have shifted in recent years. For example, individuals might become more politically active—and therefore more willing to donate—during contentious elections, while economic uncertainty could make donors more hesitant to give. Regardless of these contextual differences, we will explore how the researchers designed and analyzed their experiment, and use modern statistical techniques to replicate their findings."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#introduction",
    "href": "projects/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was one of the largest of its kind. Notably, it was conducted for a politically motivated nonprofit using paper mail solicitations in 2005. These characteristics are important to consider, as donor behavior may have shifted in recent years. For example, individuals might become more politically active—and therefore more willing to donate—during contentious elections, while economic uncertainty could make donors more hesitant to give. Regardless of these contextual differences, we will explore how the researchers designed and analyzed their experiment, and use modern statistical techniques to replicate their findings."
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#data",
    "href": "projects/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nFirst we’ll load the data from a dta file and conduct EDA inorder to better understand the datatypes and distrubtion of data.\nWe’ll rename the columns we want to make a distribution plot of and check the distribution and check for any outliers.\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n\n\n3 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another. Looking at table 1 in the paper we see the reserachers have provided summary statistics of several features in the data. This allows us to see that the control and treatment group are split evenly among those features. Showing the split allows us to perform useful statics techniques.\nTo verify the success of random assignment, I compare pre-treatment characteristics between the treatment and control groups. Table 1 in the original study shows that the groups are similar across key variables, supporting the claim that any later differences are due to the treatment, not underlying differences.\nI replicate this by testing variables like mrm2 (months since last donation), dormant (donated earlier in 2005), and female. Both t-tests and linear regressions confirm no statistically significant differences, reinforcing that the groups are balanced and that the experimental design is valid.\n\n\n\n\n\n\nVariables To Test at the 95% CI\n\n\n\nmrm2, dormant,female\n\n\n\nWe’ll conduct a t-test with a 95% confidence interval to ensure and verify the months since last donation variable with a linear regression\n\nCreate a list of the variables we want to check\nCreate a function that we can use repedetly\nWe’ll use css styling throughout our analysis for better readability\n\n\n\nvariables_to_test = ['mrm2', 'dormant', 'female']\n\ndef t_test(data, target):\n    control_data = data[data['treatment'] == 0]\n    treatment_data = data[data['treatment'] == 1]\n\n    control_mean = control_data[target].mean()\n    treatment_mean = treatment_data[target].mean()\n\n    control_std = control_data[target].std()\n    treatment_std = treatment_data[target].std()\n\n    control_n = len(control_data[target].dropna())\n    treatment_n = len(treatment_data[target].dropna())\n\n    se = ((control_std**2 / control_n) + (treatment_std**2 / treatment_n)) ** 0.5\n    t_stat = (treatment_mean - control_mean) / se\n\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=control_n + treatment_n - 2))\n\n    significance = \"significant\" if p_value &lt; 0.05 else \"not significant\"\n    \n\n    res = pd.DataFrame([{\n        't_stat': round(t_stat, 2),\n        'control_mean': round(control_mean, 2),\n        'treatment_mean': round(treatment_mean, 2),\n        'mean_diff': round(treatment_mean - control_mean, 2),\n        'standard_error': round(se, 2),\n        'control_n': control_n,\n        'treatment_n': treatment_n,\n        'p_value':p_value,\n        'significance':significance\n    }])\n    return display(res)\nt_test_results = {}\n\n\nfrom IPython.display import display, HTML\n\nfor var in variables_to_test:\n    res = t_test(data, var)  # This displays a table already\n    html = f\"\"\"\n    &lt;div class ='balance-card'&gt;\n        &lt;h4&gt; {var} — Balance Test&lt;/h4&gt;\n        &lt;p&gt;This section shows a comparison between the treatment and control groups for &lt;code&gt;{var}&lt;/code&gt;. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.&lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n    display(HTML(html))\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n0.12\n13.0\n13.01\n0.01\n0.11\n16687\n33395\n0.904855\nnot significant\n\n\n\n\n\n\n\n\n    \n         mrm2 — Balance Test\n        This section shows a comparison between the treatment and control groups for mrm2. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.\n    \n    \n\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n0.17\n0.52\n0.52\n0.0\n0.0\n16687\n33396\n0.861961\nnot significant\n\n\n\n\n\n\n\n\n    \n         dormant — Balance Test\n        This section shows a comparison between the treatment and control groups for dormant. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.\n    \n    \n\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n-1.75\n0.28\n0.28\n-0.01\n0.0\n16339\n32633\n0.07952\nnot significant\n\n\n\n\n\n\n\n\n    \n         female — Balance Test\n        This section shows a comparison between the treatment and control groups for female. A t-test is used to determine whether the difference is statistically significant at the 95% confidence level.\n    \n    \n\n\n\n\nLinear Regression Results\nUsing a Linear regression we can verfy our results of the t-test for the mrm2 variable\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\n\n\n\n\n0\nIntercept\n12.9981\n\n\n1\nTreatment Coefficient\n0.0137"
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#experimental-results",
    "href": "projects/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nI analyzed whether matched donations led to an increased response rate of making a donation.\nTo evaluate whether matching donations increase the likelihood that someone donates, I begin by comparing the share of donors between the treatment and control groups. A barplot shows a higher proportion of donations among individuals who received a match offer. This visual evidence suggests that the match has an encouraging effect on giving behavior.\nTo statistically test this difference, I run both a t-test and a simple linear regression using the binary outcome variable gave. Both methods show a small but statistically significant increase in donation likelihood for the treatment group—confirming the results reported in Table 2A, Panel A of the original paper. This indicates that simply announcing a matching offer increases the probability that someone donates, even if the amount of the match (e.g., 1:1 or 3:1) does not change that decision.\nI also fit a probit regression, which models the probability of donating. While the coefficient is significant and consistent in sign with the linear model, the results do not numerically replicate Table 3 from the paper—likely due to rounding differences, omitted covariates, or reporting inconsistencies noted by the authors themselves. Nonetheless, the directional effect is clear: offering a match increases participation in giving.\nThese results reinforce a key behavioral insight: people are more likely to act charitably when they feel their gift is amplified, even if the actual match size doesn’t substantially change the outcome.\n\n\n\n\n\n\nPercentage of Donators by Treatment Group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid Not Give\nGave\n\n\n\n\nControl\n0.982142\n0.017858\n\n\nTreatment\n0.977961\n0.022039\n\n\n\n\n\n\n\nText(0.5, 1.0, 'Percentage of Treatment and Control Populations that Donated')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT-Test of was a donation made\n\n\n\n\n\n\n\nt_test(data, 'gave')\n\n\n\n\n\n\n\n\nt_stat\ncontrol_mean\ntreatment_mean\nmean_diff\nstandard_error\ncontrol_n\ntreatment_n\np_value\nsignificance\n\n\n\n\n0\n3.21\n0.02\n0.02\n0.0\n0.0\n16687\n33396\n0.001331\nsignificant\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbit Regression\n\n\n\n\n\n\n\nX = data[['treatment']]\ny = data['gave']\n\n# Fit Probit model\nprobit_model = sm.discrete.discrete_model.Probit(y, X).fit()\n\n\ndata['predicted_prob'] = probit_model.predict(X)\n# Extract key regression info\nprobit_df = pd.DataFrame({\n    \"Variable\": probit_model.params.index,\n    \"Coefficient\": probit_model.params.round(4).values,\n    \"Std. Error\": probit_model.bse.round(4).values,\n    \"z-Statistic\": probit_model.tvalues.round(2).values,\n    \"p-Value\": probit_model.pvalues.round(5).values\n})\n\nOptimization terminated successfully.\n         Current function value: 0.301543\n         Iterations 7\n\n\n\n\n\n\n  Probit Regression: Likelihood of Donating\n  \n\n\n\nVariable\nCoefficient\nStd. Error\nz-Statistic\np-Value\n\n\n\n\ntreatment\n-2.0134\n0.0153\n-131.73\n0.0\n\n\n\n\n  Interpretation: The treatment variable has a statistically significant effect on the probability of donating.\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess whether the size of the matching grant—specifically 1:1, 2:1, or 3:1—affects the likelihood that individuals make a donation. I restrict the analysis to individuals in the treatment group and conduct a series of t-tests comparing response rates across the different match ratios.\nThe results show no statistically significant differences in donation rates between the 1:1, 2:1, and 3:1 match offers. The mean differences in response rates are small, and p-values well above 0.05 confirm that these differences are not meaningful. This aligns with the authors’ observation on page 8 of the paper that “figures suggest that neither the match threshold nor the example amount had a meaningful influence on behavior.”\nIn short, the data suggest that it’s the presence of a match—not its magnitude—that motivates people to give. This finding has practical implications for fundraising: offering a basic match may be just as effective as offering a more generous one, potentially saving costs for the nonprofit without sacrificing donor engagement.\n\n\n\n\n\n\nMatch Rate Analysis\n\n\n\n\n\n\n\n# Filter to treatment group only\nmatched = data[data['treatment'] == 1]\n\n# Separate by match ratio\ngroup_1_1 = matched[matched['ratio'] == 1]['gave']\ngroup_2_1 = matched[matched['ratio'] == 2]['gave']\ngroup_3_1 = matched[matched['ratio'] == 3]['gave']\n\n# Perform t-tests\nresults = []\n\ndef run_ratio_ttest(group_a, group_b, label_a, label_b):\n    t_stat, p_val = ttest_ind(group_a, group_b, equal_var=False)\n    return {\n        'Group A': label_a,\n        'Group B': label_b,\n        'Mean A': round(group_a.mean(), 4),\n        'Mean B': round(group_b.mean(), 4),\n        'Mean Diff': round(group_b.mean() - group_a.mean(), 4),\n        't_stat': round(t_stat, 3),\n        'p_value': round(p_val, 4),\n        'significance': 'significant' if p_val &lt; 0.05 else 'not significant'\n    }\n\nresults.append(run_ratio_ttest(group_1_1, group_2_1, '1:1', '2:1'))\nresults.append(run_ratio_ttest(group_1_1, group_3_1, '1:1', '3:1'))\nresults.append(run_ratio_ttest(group_2_1, group_3_1, '2:1', '3:1'))\n\n# View as DataFrame\nresults_df = pd.DataFrame(results)\ndisplay(results_df)\n\n\n\n\n\n\n\n\nGroup A\nGroup B\nMean A\nMean B\nMean Diff\nt_stat\np_value\nsignificance\n\n\n\n\n0\n1:1\n2:1\n0.0207\n0.0226\n0.0019\n-0.965\n0.3345\nnot significant\n\n\n1\n1:1\n3:1\n0.0207\n0.0227\n0.0020\n-1.015\n0.3101\nnot significant\n\n\n2\n2:1\n3:1\n0.0226\n0.0227\n0.0001\n-0.050\n0.9600\nnot significant\n\n\n\n\n\n\n\nTo complement the t-tests, I also run a regression to assess the effect of match size on donation behavior. I restrict the sample to the treatment group and regress the binary outcome gave on indicator variables for the match ratios: 2:1 and 3:1, with 1:1 serving as the reference group. This specification allows us to estimate how each match ratio affects the likelihood of giving relative to the 1:1 baseline.\nThe regression coefficients for both 2:1 and 3:1 match ratios are small and not statistically significant. This mirrors the t-test findings and supports the paper’s conclusion that larger match ratios do not meaningfully increase participation. The statistical precision of the estimates is low, with wide confidence intervals crossing zero, reinforcing that any observed differences could plausibly be due to chance. These findings suggest that while match offers increase overall response rates, increasing the match beyond 1:1 yields no additional benefit.\n\n\n\n\n\n\nRatio Analysis\n\n\n\n\n\n\n\nmatched = data[data['treatment'] == 1].copy()\n\nmatched['ratio1'] = (matched['ratio'] == 1).astype(int)\nmatched['ratio2'] = (matched['ratio'] == 2).astype(int)\nmatched['ratio3'] = (matched['ratio'] == 3).astype(int)\nimport statsmodels.api as sm\n\nX = matched[['ratio2', 'ratio3']]  # ratio1 is the omitted (reference) group\nX = sm.add_constant(X)\ny = matched['gave']\n\nmodel = sm.OLS(y, X).fit()\n\nimport pandas as pd\nfrom IPython.display import HTML\n\n# Extract and clean key regression results\ntable = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.round(4).values,\n    \"Std. Error\": model.bse.round(4).values,\n    \"t-Statistic\": model.tvalues.round(2).values,\n    \"p-Value\": model.pvalues.round(5).values\n})\n\ntable['Variable'] = table['Variable'].replace({'const': 'Intercept'})\n\n\n\n\n\n  OLS Regression: Effect of Match Ratios on Giving\n  \n\n\n\nVariable\nCoefficient\nStd. Error\nt-Statistic\np-Value\n\n\n\n\nIntercept\n0.0207\n0.0014\n14.91\n0.00000\n\n\nratio2\n0.0019\n0.0020\n0.96\n0.33828\n\n\nratio3\n0.0020\n0.0020\n1.01\n0.31332\n\n\n\n\n  Note: Reference group is ratio = 1:1.\n\n\n\nTo further examine the effectiveness of different match sizes, I calculate the difference in response rates directly from the data. The increase in giving from a 1:1 to a 2:1 match is approximately 0.19 percentage points, and from 2:1 to 3:1 is just 0.01 percentage points—both very small changes. I then compare these results to the fitted coefficients from the earlier regression, which also show similarly small and statistically insignificant differences between the match levels.\nTaken together, these results suggest that raising the match ratio does not meaningfully affect the likelihood of donating. Whether donors are offered a 1:1, 2:1, or 3:1 match, their behavior appears largely unchanged. This reinforces the paper’s central finding: the presence of a match matters, but its size does not.\n\n\n\n\n\n\nMatch Rate Regression Analysis\n\n\n\n\n\n\n\n# Filter for treatment group\nmatched = data[data['treatment'] == 1]\n\n# Calculate average donation rates\nmean_1_1 = matched[matched['ratio'] == 1]['gave'].mean()\nmean_2_1 = matched[matched['ratio'] == 2]['gave'].mean()\nmean_3_1 = matched[matched['ratio'] == 3]['gave'].mean()\n\n# Differences in means\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# OLS Regression: ratio as categorical\nmodel = smf.ols('gave ~ C(ratio)', data=matched).fit()\n\n# Extract coefficients\ncoef_2_1 = model.params['C(ratio)[T.2]']\ncoef_3_1 = model.params['C(ratio)[T.3]']\n\n# Difference between model coefficients\ndiff_model_2_1_vs_1_1 = coef_2_1\ndiff_model_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n\n\n\n\n  Differences in Response Rates by Match Ratio\n  \n    Raw Mean Difference (2:1 vs 1:1): 0.0019\n    Raw Mean Difference (3:1 vs 2:1): 0.0001\n    Model Coefficient (2:1 vs 1:1): -1229429281.1791\n    Model Coefficient (3:1 vs 2:1): -0.0001\n  \n  Note: These results suggest no statistically meaningful differences in response rate between 1:1, 2:1, and 3:1 match ratios.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nTo test whether the match offer affects how much donors give, I run a t-test and a simple linear regression comparing donation amounts between the treatment and control groups. This analysis includes all individuals, whether or not they donated. The results show a small increase in average donation amount in the treatment group, but the difference is only marginally statistically significant. This suggests that while match offers may slightly increase total donations on average, the effect is likely driven by more people giving—not by individuals giving more. This finding aligns with the broader conclusion that matching incentives affect participation rather than generosity.\n\n\n\n\n\n\nDonation Size Analysis\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Separate groups\ncontrol = data[data['treatment'] == 0]['amount']\ntreatment = data[data['treatment'] == 1]['amount']\n\n# T-test\nt_stat, p_value = ttest_ind(treatment, control, equal_var=False)\n\nprint(f\"t = {t_stat:.3f}, p = {p_value:.4f}\")\n\nt = 1.918, p = 0.0551\n\n\nTo isolate the effect of the treatment on the size of the donation among those who actually gave, I limit the sample to donors only and rerun the regression. This allows us to assess whether the match offer influenced how much people gave, conditional on deciding to donate. The coefficient on the treatment variable is small and statistically insignificant, indicating that among donors, those who received a match offer gave approximately the same amount as those who did not. These results suggest that the match influences whether someone donates, but not how much they give once they decide to contribute. Because the treatment was randomly assigned, the coefficient does have a causal interpretation—but here, the causal effect on donation size appears to be negligible.\n\n\n\n\n\n\nRatio Analysis\n\n\n\n\n\n\n\ndonors = data[data['amount'] &gt; 0]\nimport statsmodels.api as sm\n\nX = sm.add_constant(donors['treatment'])  # 1 if matched, 0 if not\ny = donors['amount']\n\nmodel = sm.OLS(y, X).fit()\n# Extract and clean results\nols_df = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.round(4).values,\n    \"Std. Error\": model.bse.round(4).values,\n    \"t-Statistic\": model.tvalues.round(2).values,\n    \"p-Value\": model.pvalues.round(5).values\n})\n\nols_df[\"Variable\"] = ols_df[\"Variable\"].replace({'const': 'Intercept'})\nhtml_table = ols_df.to_html(index=False, classes='table table-sm table-striped', border=0)\n\nThe below graph illustrates visually that were was not a staistically signifcant delta between the contorl and treatment group in our sample.\n\n\n\n\n\n\nPlot Analysis\n\n\n\n\n\n\n\n# Filter to people who donated\ndonors = data[data['amount'] &gt; 0]\ncontrol_donors = donors[donors['treatment'] == 0]\ntreatment_donors = donors[donors['treatment'] == 1]\n\n# Calculate means\ncontrol_mean = control_donors['amount'].mean()\ntreatment_mean = treatment_donors['amount'].mean()"
  },
  {
    "objectID": "projects/hw1/hw1_questions.html#simulation-experiment",
    "href": "projects/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\n\n\n\n\nSimulation Using Bernoullui Distrubtion\n\n\n\n\n\n\nFrom out bernoulii parameters we can caclulate the mean and the stndard deviation of the distrubtion\n\np_no =.018\nmu_no_char = p_no\nstd_no_char = (mu_no_char* (1-mu_no_char))**.5\n\np = .022\nmu_char = p\nstd_char =  (mu_char * (1-mu_char))**.5\n\n\n\nThe mean and standard deviation for the respondents who did not receive a donation match is (0.018, 0.13)\nThe mean and standard deviation for the respondents who did receive a donation match is (0.022, 0.15)\n\n\n\nLaw of Large Numbers\nThe Law of Large numbers say the more information we have the better our estimates will be. In statics terminology as we increase our sample size the sample mean will move closer to the population mean. To see this law in action we’ll increase our sample size to 10,00 and calcuate the average as we increase the sample size. This should allign with the means that we know from our benoulli distrbution.\n\nn = 10_000\nnp.random.seed(38)\nno_match_draws = np.random.binomial(1, p_no, size=n)\nmatch_draws = np.random.binomial(1, p, size=n)\ndifferences = match_draws - no_match_draws\ncumulative_avg = np.cumsum(differences) / np.arange(1, 10001)\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\nCentral Limit Theorem Demo\n\n\n\n\n\n\nThe central limit theorem states that as we increase the sample size of a sample the distrubtion of the samples will become more like the bell-shaped cureve, regardless of the starting distrubtion. To Test this this we’ll create 4 sample sizes from our given bernoulli distribtuion. We should expect to see the central limit theorem ‘kick-in’ and the distribtuion start to become more bellshaped.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#disibutin parameters\np_no = 0.018\np_match = 0.022\n\n#sample sizes \nsample_sizes = [50, 200, 500, 1000]\n#number of simluatoin of a sample size\nn_sim = 1000\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\n\n#loop through the different sample size\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_sim):\n        #pull a no_match from a binomial distrubtion of size 1000\n        no_match = np.random.binomial(1, p_no, size=n)\n        #pull a match from a binomial distrubtion of size 1000\n        match = np.random.binomial(1, p_match, size=n)\n        #append the differences to a running list per sample size\n        diff = match.mean() - no_match.mean()\n        diffs.append(diff)\n\n    # plot the graphs\n    axs[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(p_match - p_no, color='red', linestyle='--', label='True Difference')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Mean Difference (Match - No Match)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Histograms of Sample Mean Differences — Central Limit Theorem in Action\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Homework",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n \n\n\n\n\n\nMay 24, 2025\nLowell Capobianco\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n \n\n\n\n\n\nMay 24, 2025\nLowell C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n \n\n\n\n\n\nApr 18, 2025\nLowell Capobianco\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Personnel/Reddit_Sentiment/Reddit_Scrape.html",
    "href": "Personnel/Reddit_Sentiment/Reddit_Scrape.html",
    "title": "Begin sentiment Analysis",
    "section": "",
    "text": "import praw\nimport regex as re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport os\nimport sklearn\nimport nltk\n\nMatplotlib created a temporary cache directory at /tmp/matplotlib-rtvczs6a because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\nclientid = ''\nclient_secret = ''\nuser_agent = \"\"\nreddit = praw.Reddit(\n    client_id=clientid,\n    client_secret=client_secret,\n    user_agent=user_agent\n)\nprint(reddit.read_only)\n# Output: True\n\nTrue\nimport re\n\ndef find_matched_terms(text, terms):\n    matched = []\n    for term in terms:\n        # Word-boundary search (e.g., matches \"lead\" but not \"leadership\")\n        pattern = rf'\\b{re.escape(term)}\\b'\n        if re.search(pattern, text, flags=re.IGNORECASE):\n            matched.append(term)\n    return matched\ntar_sub = 'datacenter'\nbattery_terms = ['battery', 'batteries', 'lithium', 'li-ion', 'sodium', 'lead', 'acid', 'ups','sodium','ion']\ntime_filter = 'all'\ndata = []\nseen_comments = set()\n\nfor term in battery_terms:\n    for submission in reddit.subreddit(tar_sub).search(term, sort=\"top\", time_filter=time_filter):\n        submission.comments.replace_more(limit=0)\n        for comment in submission.comments.list():\n            if comment.id in seen_comments:\n                continue\n            text = comment.body.lower()\n            matched_terms = find_matched_terms(text, battery_terms)\n            if matched_terms:\n                data.append({\n                    'comment_id': comment.id,\n                    'author': str(comment.author),\n                    'text': text,\n                    'matched_terms': matched_terms,\n                    'submission_id': submission.id,\n                    'submission_title': submission.title\n                })\n                seen_comments.add(comment.id)\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n1\ng0diub1\nRedebo\ndisconnect the battery strings via the associa...\n[battery, batteries, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n2\ng0dtxqg\nletsbebuns\nthe batteries should be removed based on age. ...\n[batteries, lead, acid, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n3\ng0ebpdx\nlooktowindward\nyes, they are a safety hazard. you should call...\n[battery]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n\n\n4\ng0euygg\nxpkranger\ni’m going to get rid of them, waiting on a quo...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nexample = df['text'][1]\nprint(example)\n\nex_analyzer = SentimentIntensityAnalyzer()\n\nex_sent = ex_analyzer.polarity_scores(example)\nprint(ex_sent)\n\ndisconnect the battery strings via the associated battery breaker or switch that is either in the ups or electrically in line with the ups.  if you want to be safer after that, remove the intercell jumpers between each battery (caution, batteries are always 'live' and can present voltage and current when both terminals are touched).  once the batteries are disconnected and just sitting on a shelf, you can leave them there practically forever with no risk.\n\nalso, would definitely recommend rip and replace on a 15 year old ups.  if you are located in the western us, my company can assist if you're in need.\n{'neg': 0.011, 'neu': 0.889, 'pos': 0.1, 'compound': 0.8366}\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nanalyzer = SentimentIntensityAnalyzer()\n# Apply VADER to each comment's text\ndf['sentiment'] = df['text'].apply(lambda x: analyzer.polarity_scores(x))\ndf = pd.concat([df.drop('sentiment', axis=1), df['sentiment'].apply(pd.Series)], axis=1)\ndef get_label(score):\n    if score &gt;= 0.05:\n        return 'positive'\n    elif score &lt;= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\ndf['sentiment_label'] = df['compound'].apply(get_label)\ndf.head(3)\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\nneg\nneu\npos\ncompound\nsentiment_label\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.084\n0.873\n0.043\n-0.4215\nnegative\n\n\n1\ng0diub1\nRedebo\ndisconnect the battery strings via the associa...\n[battery, batteries, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.011\n0.889\n0.100\n0.8366\npositive\n\n\n2\ng0dtxqg\nletsbebuns\nthe batteries should be removed based on age. ...\n[batteries, lead, acid, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.029\n0.971\n0.000\n-0.2617\nnegative\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Simulate a sample of the df for visualization\ndf_viz = pd.DataFrame({\n    'matched_terms': df['matched_terms'],\n    'sentiment_label': df['sentiment_label']\n})\ndf_exploded = df_viz.explode('matched_terms')\n\n\n# Explode matched_terms to allow grouping by single term\ndf_exploded = df_viz.explode('matched_terms')\n\n# Count sentiment labels per battery type\nsentiment_counts = df_exploded.groupby(['matched_terms', 'sentiment_label']).size().unstack(fill_value=0)\n\n# Plot the sentiment distribution per battery type\nsentiment_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Sentiment Distribution by Battery Type')\nplt.xlabel('Battery Type')\nplt.ylabel('Number of Comments')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.grid(axis='y')\n\nplt.show()\ndf\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\nneg\nneu\npos\ncompound\nsentiment_label\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.084\n0.873\n0.043\n-0.4215\nnegative\n\n\n1\ng0diub1\nRedebo\ndisconnect the battery strings via the associa...\n[battery, batteries, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.011\n0.889\n0.100\n0.8366\npositive\n\n\n2\ng0dtxqg\nletsbebuns\nthe batteries should be removed based on age. ...\n[batteries, lead, acid, ups]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.029\n0.971\n0.000\n-0.2617\nnegative\n\n\n3\ng0ebpdx\nlooktowindward\nyes, they are a safety hazard. you should call...\n[battery]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.000\n0.718\n0.282\n0.6705\npositive\n\n\n4\ng0euygg\nxpkranger\ni’m going to get rid of them, waiting on a quo...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.000\n0.807\n0.193\n0.7351\npositive\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n263\nfrhl2ew\nswedishhat\nto be fair, full 19in eia racks with ups's are...\n[ups]\nfemyhi\nDoes anyone here use OCP hardware in their dat...\n0.000\n0.944\n0.056\n0.7391\npositive\n\n\n264\nkih1o5a\nNone\n&gt; finally adding your future generator? ah ha ...\n[lead]\n199drs1\nHow can a Field Controls Engineer get PLC Prog...\n0.056\n0.892\n0.052\n-0.1779\nnegative\n\n\n265\ng9583fq\nRedebo\nyou might ask your provider if they have some ...\n[battery]\njcvdpk\nFloor protection in data center\n0.000\n0.940\n0.060\n0.2732\npositive\n\n\n266\nmmhd66m\nAlligatorDan\nthe engineering mindset on youtube is a great ...\n[ups]\n1jw4mql\nAmazon DCEO\n0.013\n0.827\n0.160\n0.9764\npositive\n\n\n267\nmmgzvfn\nLucky_Luciano73\namazon has hired ex-navy guys that we work wit...\n[ups]\n1jw4mql\nAmazon DCEO\n0.092\n0.718\n0.190\n0.6486\npositive\n\n\n\n\n268 rows × 11 columns"
  },
  {
    "objectID": "Personnel/Reddit_Sentiment/Reddit_Scrape.html#model-is-being-trained-on-the-twitter-data",
    "href": "Personnel/Reddit_Sentiment/Reddit_Scrape.html#model-is-being-trained-on-the-twitter-data",
    "title": "Begin sentiment Analysis",
    "section": "model is being trained on the twitter data",
    "text": "model is being trained on the twitter data\n\nMODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\n\nencoded_text = tokenizer(example,return_tensors='pt')\noutput = model(**encoded_text)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n\nscores_dict = {\n    'roberta_neg': scores[0],\n    'roberta_neutral': scores[1],\n    'roberta_post': scores[2]\n}\n\n\nprint(scores_dict)\nprint(ex_sent)\n\n{'roberta_neg': 0.07787177, 'roberta_neutral': 0.6052092, 'roberta_post': 0.31691906}\n{'neg': 0.011, 'neu': 0.889, 'pos': 0.1, 'compound': 0.8366}\n\n\n\nfrom scipy.special import softmax\n\ndef polarity_scores_roberta(text):\n    encoded_text = tokenizer(\n        text,\n        return_tensors='pt',\n        truncation=True,       # THIS is what prevents the crash\n        max_length=512,\n        padding=True\n    )\n    output = model(**encoded_text)\n    scores = output[0][0].detach().numpy()\n    scores = softmax(scores)\n    labels = ['negative', 'neutral', 'positive']\n    return labels[scores.argmax()], scores.max()\n\n\ndf[['roberta_label', 'roberta_score']] = df['text'].apply(\n    lambda x: pd.Series(polarity_scores_roberta(x))\n)\n\n\ndf.head(1)\n\n\n\n\n\n\n\n\ncomment_id\nauthor\ntext\nmatched_terms\nsubmission_id\nsubmission_title\nneg\nneu\npos\ncompound\nsentiment_label\nroberta_label\nroberta_score\n\n\n\n\n0\ng0bu3y5\nghostalker47423\ndo a visual inspection, and if you see any lea...\n[batteries]\ni3jdwl\nInheriting a 30 kVA UPS with 2 strings of 24 b...\n0.084\n0.873\n0.043\n-0.4215\nnegative\nnegative\n0.458596\n\n\n\n\n\n\n\n\ndf_exploded = df.explode('matched_terms')\n\n\n# Group by label to visualize distribution\nlabel_counts = df['roberta_label'].value_counts()\n\n# Plot sentiment label distribution\nplt.figure(figsize=(6, 4))\nlabel_counts.plot(kind='bar', color='skyblue')\nplt.title(\"RoBERTa Sentiment Label Distribution\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n# Plot average sentiment score by battery type\navg_score_by_term = df_exploded.groupby('matched_terms')['roberta_score'].mean().sort_values()\n\nplt.figure(figsize=(8, 5))\navg_score_by_term.plot(kind='barh', color='salmon')\nplt.title(\"Average RoBERTa Sentiment Score by Battery Term\")\nplt.xlabel(\"Average Score\")\nplt.ylabel(\"Battery Term\")\nplt.grid(axis='x')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ---------------------------\n# Visualization by Sentiment Class and Term\n# ---------------------------\nsentiment_counts = df_exploded.groupby(['matched_terms', 'roberta_label']).size().unstack(fill_value=0)\n\nplt.figure(figsize=(10, 6))\nsentiment_counts.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(10, 6))\nplt.title(\"Sentiment Distribution per Battery Term\")\nplt.xlabel(\"Battery Term\")\nplt.ylabel(\"Number of Comments\")\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n# ---------------------------\n# Drill-down: Most Positive & Negative per Term\n# ---------------------------\nmost_pos_comments = df_exploded.sort_values(by='roberta_score', ascending=False).groupby('matched_terms').first().reset_index()\nmost_neg_comments = df_exploded.sort_values(by='roberta_score').groupby('matched_terms').first().reset_index()\n\n# Combine for review\ndrill_df = pd.merge(\n    most_pos_comments[['matched_terms', 'text', 'roberta_score']],\n    most_neg_comments[['matched_terms', 'text', 'roberta_score']],\n    on='matched_terms',\n    suffixes=('_most_positive', '_most_negative')\n)\ndisplay(drill_df)\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_terms\ntext_most_positive\nroberta_score_most_positive\ntext_most_negative\nroberta_score_most_negative\n\n\n\n\n0\nacid\nactually, many ups', especially dc grade ups',...\n0.859889\n\"no such thing as a data center that's too col...\n0.455056\n\n\n1\nbatteries\nthat'll prove to be a disaster if the batterie...\n0.945713\nthis may have been the consensus 5 years ago.\\...\n0.436103\n\n\n2\nbattery\nare all of the raid batteries the same age? an...\n0.913489\ni handle all field ups units for a large truck...\n0.446345\n\n\n3\nion\nyes, it is detailed in their site that the mv ...\n0.865184\n\"no such thing as a data center that's too col...\n0.455056\n\n\n4\nlead\nfirst of all, taking on those data center move...\n0.944411\nit really depends op, working at a dc is great...\n0.407846\n\n\n5\nli-ion\nyes, it is detailed in their site that the mv ...\n0.865184\nof course it's feasible. tesla made mega batte...\n0.643169\n\n\n6\nlithium\nactually, many ups', especially dc grade ups',...\n0.859889\nthis may have been the consensus 5 years ago.\\...\n0.436103\n\n\n7\nups\nthe engineering mindset on youtube is a great ...\n0.973194\nyeah i wish we got more remote hands tickets a...\n0.426720\n\n\n\n\n\n\n\n\nneg_comments = df_exploded[df_exploded['roberta_label'] == 'negative']\nneg_lithium = neg_comments[neg_comments['matched_terms'] == 'lithium']\n\n\n\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')  # keeps only words, removes punctuation\n\ndef extract_keywords_no_punkt(text_series):\n    all_words = []\n    for text in text_series:\n        tokens = tokenizer.tokenize(text.lower())\n        filtered = [word for word in tokens if word not in stop_words]\n        all_words.extend(filtered)\n    return Counter(all_words).most_common(20)\n\n\n# Define stop_words using NLTK's built-in list\nfrom nltk.corpus import stopwords\n\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Now that stop_words is defined, rerun the visualization block\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndef extract_keywords_no_punkt(text_series):\n    all_words = []\n    for text in text_series:\n        tokens = tokenizer.tokenize(text.lower())\n        filtered = [word for word in tokens if word not in stop_words]\n        all_words.extend(filtered)\n    return Counter(all_words).most_common(20)\n\nchemistry_pain_points = {}\n\nbattery_terms = neg_comments['matched_terms'].unique()\n\nfor term in battery_terms:\n    chem_comments = neg_comments[neg_comments['matched_terms'] == term]\n    keywords = extract_keywords_no_punkt(chem_comments['text'])\n    chemistry_pain_points[term] = keywords\n\npain_point_data = []\n\nfor term, keywords in chemistry_pain_points.items():\n    for word, freq in keywords:\n        pain_point_data.append({'battery_chemistry': term, 'keyword': word, 'frequency': freq})\n\npain_point_df = pd.DataFrame(pain_point_data)\n\n# Visualize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntop_pain_points = (\n    pain_point_df\n    .sort_values(by='frequency', ascending=False)\n    .groupby('battery_chemistry')\n    .head(5)\n)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(\n    data=top_pain_points,\n    y='keyword',\n    x='frequency',\n    hue='battery_chemistry',\n    dodge=False\n)\nplt.title('Top Pain Point Keywords by Battery Chemistry')\nplt.xlabel('Frequency')\nplt.ylabel('Keyword')\nplt.legend(title='Battery Chemistry', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(axis='x')\nplt.show()\n\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  }
]