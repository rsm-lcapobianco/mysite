---
title: "Poisson Regression Examples"
author: "Lowell Capobianco"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
format:
    html:
        css: ../../styles.css
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data


```{python}
# | echo: false
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings('ignore')
airbnb = pd.read_csv('airbnb.csv')
blueprinty = pd.read_csv('blueprinty.csv')
```

::: {.callout-tip appearance="simple" icon=false  title="EDA"}
:::
```{python}
blueprint_hist = pd.pivot_table(blueprinty, index='iscustomer', values='patents', aggfunc='mean').head()
display(blueprint_hist)

blueprint_hist = blueprint_hist.reset_index()
```
```{python}
# | echo: false
# Create a histogram-style bar chart
plt.figure(figsize=(6, 4))
plt.bar(blueprint_hist['iscustomer'].astype(str), blueprint_hist['patents'], edgecolor='black')

# Add labels and title
plt.xlabel('Is Customer')
plt.ylabel('Average Number of Patents')
plt.title('Average Patents by Customer Status')

# Show the plot
plt.tight_layout()
plt.show()
```
On average if a firm is a customer they have slightly more patents then a noncustomer 


Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
blueprinty['age_bins']= pd.cut(blueprinty['age'], bins=[0, 20, 30, 40, 50], right=False)
blueprint_hist_age =pd.crosstab(index=blueprinty['age_bins'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')
display(blueprint_hist_age)
```
```{python}
# | echo: false
plot_data_age = blueprint_hist_age.drop(index='Total', columns='Total')
# Plot
plot_data_age.plot(
    kind='bar',
    stacked=False,
    figsize=(8, 5),
    edgecolor='black',
    color =( 'skyblue','green',),
)
# Add labels and title
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.title('Customer Status by Age Group')
plt.legend(title='Is Customer', labels=['No (0)', 'Yes (1)'])
plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```
```{python}
blueprint_hist_region= pd.crosstab(index=blueprinty['region'], columns=blueprinty['iscustomer'],margins=True,margins_name='Total')
display(blueprint_hist_region)
```
```{python}
# | echo: false
plot_data_region = blueprint_hist_region.drop(index='Total', columns='Total')

# Plot
plot_data_region.plot(
    kind='bar',
    stacked=False,
    figsize=(8, 5),
    edgecolor='black',
    color=['skyblue','forestgreen']
)
# Add labels and title
plt.xlabel('Region')
plt.ylabel('Count')
plt.title('Customer Status by Region')
plt.legend(title='Is Customer', labels=['No (0)', 'Yes (1)'])
plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```


Most Firms are in the 20 - 30 age range. Additoinaly there is a disporpoiante amount of customers who are in the the North East region. We may see a higher  coeffcient weight on these 2 characteristcs due to them being overrepresented in the data.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.


::: {.callout-tip appearance="simple" icon=false  title="Log Likehood for Poission"}
:::

### Deriving the Likelihood Function for the Poisson Distribution

####  Step 1: Poisson Distribution for a Single Observation


The probability of observing a count $( y_i )$ for observation $( i )$, given Poisson rate \ $( \lambda_i )$, is:


$$
P(Y_i = y_i \mid \lambda_i) = \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!}
$$

#### Step 2: Likelihood Function for Independent Observations

Assuming we observe $( n )$ independent data points $( y_1, y_2, \ldots, y_n )$, each with rate $( \lambda_i )$, the joint likelihood is the product of the individual probabilities:
$$
\mathcal{L}(\lambda_1, \ldots, \lambda_n) = \prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!}
$$

#### Step 3: Log-Likelihood Function

Taking the natural logarithm of the likelihood simplifies the product into a sum:
$$
\log \mathcal{L} = \sum_{i=1}^{n} \left( -\lambda_i + y_i \log(\lambda_i) - \log(y_i!) \right)
$$
This is the **log-likelihood function** for the Poisson model.



::: {.callout-tip appearance="simple" icon=false  title="Log likelihood in code"}
:::




```{python}

import numpy as np
from math import factorial

def poisson_distro(lmbda, y):
    return (np.exp(-lmbda) * (lmbda ** y)) / factorial(y)

def poisson_likelihood(lmbda, y_array):
    return np.prod([poisson_distro(lmbda, y_i) for y_i in y_array])

from scipy.special import gammaln

def poisson_log_likelihood(lmbda, y_array):
    y_array = np.array(y_array)
    return np.sum(-lmbda + y_array * np.log(lmbda) - gammaln(y_array + 1))

```

We changed from the factorial to the gammln function in the log likelihood function. When using factrioal numbers get extremely large and could cause overflow errors. 
Gammaln works like an integral of the factorial function which will make the math less computatiaonlly expensivve

```{python}
# Evaluate log-likelihoods across lambda values
lambda_vals = np.arange(1, 20)
log_likelihoods = [poisson_log_likelihood(lmbda, blueprinty['patents'].values) for lmbda in lambda_vals]
```
```{python}
# | echo: false
plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, log_likelihoods, marker='o', color='navy')
plt.title("Poisson Log-Likelihood vs. Lambda")
plt.xlabel("Lambda (Î»)")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.tight_layout()
plt.show()
```


---

####  Step 1: Log-Likelihood Function

The log-likelihood for the entire sample is:

$$
\log \mathcal{L}(\lambda) = \sum_{i=1}^{n} \left( -\lambda + y_i \log(\lambda) - \log(y_i!) \right)
$$

Simplify:

$$
\ell(\lambda) = -n\lambda + \left( \sum_{i=1}^n y_i \right) \log(\lambda) - \sum_{i=1}^n \log(y_i!)
$$

---

####  Step 2: Take the Derivative w.r.t. $( \lambda )$

To find the MLE, take the derivative and set it equal to zero:

$$
\frac{d}{d\lambda} \ell(\lambda) = -n + \frac{\sum y_i}{\lambda}
$$

Set the derivative to zero:

$$
-n + \frac{\sum y_i}${\lambda} = 0
$$

---

####  Step 3: Solve


$$
\frac{\sum y_i}{\lambda} = n
\quad \Rightarrow \quad
\lambda = \frac{1}{n} \sum y_i
= \bar{y}
$$

So the **MLE** for $( \lambda )$ in a Poisson distribution is the **sample mean**:

$$
\hat{\lambda}_{\text{MLE}} = \bar{y}
$$

```{python}
from scipy.optimize import minimize_scalar
from scipy.special import gammaln
# ecause scipy.optimize minimizes by default, we minimize the negative log-likelihood

#Trick the minimze function into finding the maximum log likelhood with a negative
   #lambda lmbda --> Keep calling the function with lambda as the lmbda value
objective = lambda lmbda: -poisson_log_likelihood(lmbda, blueprinty['patents'].values)

# Perform the optimization using bounded scalar minimization
result = minimize_scalar(objective, bounds=(0.01, 20), method='bounded')

# Output the MLE estimate for lambda
lambda_mle = result.x
log_likelihood_at_mle = -result.fun

```

```{python}
# | echo: false
from IPython.display import HTML

lambda_mle_ans =lambda_mle
log_likelihood_at_mle_ans = log_likelihood_at_mle
sample_mean = blueprinty['patents'].mean()

HTML(f"""
<div class = 'result-box'>
  <p><strong> MLE for &lambda;:</strong> {lambda_mle_ans:.4f}</p>
  <p><strong> sample mean :</strong> {sample_mean:.4f}</p>
  <p><strong> Log-Likelihood at MLE:</strong> {log_likelihood_at_mle_ans:.2f}</p>
</div>
""")

```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.


We assume the Poisson rate parameter $( \lambda )$ varies by observation based on covariates $( X_i )$ and a parameter vector $( \beta )$:

$$
\lambda_i = \exp(X_i^\top \beta)
$$

This ensures $( \lambda_i > 0 )$ for all $( i )$, as required for Poisson distributions. The exponential function is the canonical inverse link function for Poisson regression.

---

### Log-Likelihood Function for Poisson Regression

Given the model $( Y_i \sim \text{Poisson}(\lambda_i) )$, the log-likelihood function across all observations is:

$$
\ell(\beta) = \sum_{i=1}^n \left[ -\exp(X_i^\top \beta) + y_i (X_i^\top \beta) - \log(y_i!) \right]
$$

This is the function we will maximize to estimate $( \beta )$ using maximum likelihood estimation.



```{python}
def poisson_log_likelihood_regression(beta, X, y):
    # Ensure all inputs are NumPy arrays
    if not isinstance(beta, np.ndarray):
        beta = np.asarray(beta)
    if not isinstance(X, np.ndarray):
        X = np.asarray(X)
    if not isinstance(y, np.ndarray):
        y = np.asarray(y)

    eta = X @ beta
    lambda_ = np.exp(eta)

    return np.sum(-lambda_ + y * eta - gammaln(y + 1))
```



```{python}
from scipy.optimize import minimize
blueprinty['age_scaled'] = (blueprinty['age'] - blueprinty['age'].mean()) / blueprinty['age'].std()
blueprinty['age_squared'] = blueprinty['age_scaled'] ** 2



blueprinty['intercept'] = 1
encoded_region = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)
blueprinty = pd.concat([blueprinty, encoded_region], axis=1)

X = blueprinty[['intercept', 'age_scaled', 'age_squared', 'region_Northeast', 'region_Northwest',
       'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()

y = blueprinty['patents'].values
initial_beta = np.zeros(X.shape[1])
result = minimize(
    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),
    x0=initial_beta,
    method='BFGS'
)
beta_mle = result.x
log_lik_at_mle = -result.fun
hessian_inv = result.hess_inv
standard_errors = np.sqrt(np.diag(hessian_inv))


```
```{python}
# | echo: false

column_names = ['intercept', 'age_scaled', 'age_squared',
                'region_Northeast', 'region_Northwest',
                'region_South', 'region_Southwest', 'iscustomer']

coef_df = pd.DataFrame({
    'Variable': column_names,
    'Estimate (MLE)': [f"{val:.4f}" for val in beta_mle],
    'Std. Error': [f"{se:.4f}" for se in standard_errors]
})

# Display formatted HTML table
display(HTML(coef_df.to_html(index=False)))

# Display log-likelihood box
HTML(f"""
<div class='result-box'>
  <p><strong>Log-Likelihood at MLE:</strong> {log_lik_at_mle:.2f}</p>
</div>
""")
```

```{python}

import statsmodels.api as sm
# Use the same standardized/scaled features
X_sm = blueprinty[['age_scaled', 'age_squared', 'region_Northeast',
                   'region_Northwest', 'region_South', 'region_Southwest', 'iscustomer']].astype(float).to_numpy()

# Add intercept (statsmodels handles it with sm.add_constant)
X_sm = sm.add_constant(X_sm)

y_sm = blueprinty['patents']
model = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())
result = model.fit()

print(result.summary())

```


The strongest effect is from iscustomer 
Age has a mild downward effect 
Region effects are minor 

```{python}

# Step 1: Copy X and set iscustomer to 0 and 1 for every firm
X_0 = X.copy()
X_1 = X.copy()

iscustomer_index = column_names.index("iscustomer")  # Adjust if you know the column index directly
X_0[:, iscustomer_index] = 0
X_1[:, iscustomer_index] = 1

# Step 2: Predict lambda (expected # patents) for both scenarios
y_pred_0 = np.exp(X_0 @ beta_mle)
y_pred_1 = np.exp(X_1 @ beta_mle)

# Step 3: Estimate average treatment effect of being a customer
avg_effect = np.mean(y_pred_1 - y_pred_0)

```

```{python}
# | echo: false
print(f"Estimated average effect of using Blueprinty's software on patent success: {avg_effect:.4f}")

```


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._


```{python}
airbnb['price_bins'] = pd.cut(airbnb['price'], bins=[0,500,5000, 10000], right=False)
airbnb['bathrooms_bins'] = pd.cut(airbnb['bathrooms'], bins=[0, 1, 2, 3, 4, 5, 6], right=False)
airbnb['bedrooms_bins'] = pd.cut(airbnb['bedrooms'], bins=[0, 1, 2, 3, 4, 5, 6], right=False)
airbnb['number_of_reviews_bins'] = pd.cut(airbnb['number_of_reviews'], bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], right=False)
```

```{python}
# | echo: false
bathroom_hist = pd.pivot_table(airbnb, index='bathrooms_bins', values='number_of_reviews', aggfunc=['mean'],margins=True,margins_name='Total')
display(bathroom_hist)
plot_data_bathrooms = bathroom_hist.drop(index='Total')

# Plot
plot_data_bathrooms.plot(
    kind='bar',
    stacked=False,
    figsize=(8, 5),
    edgecolor='black',
    color =('green',),
)

# Add labels and title
plt.xlabel('NR of Bathrooms')
plt.ylabel('Average Number of Reviews')
plt.title('Average Number of Reviews by Bathrooms')
plt.legend().set_visible(False)
plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

```{python}
# | echo: false
price_hist = pd.pivot_table(airbnb, index='price_bins', values='number_of_reviews', aggfunc=['mean'],margins=True,margins_name='Total')
display(bathroom_hist)
plot_data_price = price_hist.drop(index='Total')

# Plot
plot_data_price.plot(
    kind='bar',
    stacked=False,
    figsize=(8, 5),
    edgecolor='black',
    color =('green',),
)

# Add labels and title
plt.xlabel('price_bins')
plt.ylabel('Average Number of Reviews')
plt.title('Average Number of Reviews by Price')
plt.legend().set_visible(False)
plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

```{python}
airbnb = airbnb[airbnb['price'] <= 500]
```

```{python}
# | echo: false
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.api import GLM
from statsmodels.genmod.families import Poisson

# Step 1: Exploratory Data Analysis (EDA)
```

Drop rows with missing values across all relevant columns (before plotting)

```{python}

eda_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms', 'days']
airbnb_eda = airbnb.dropna(subset=eda_cols)
```

```{python}
# | echo: false
# Distribution of the number of reviews
sns.histplot(airbnb_eda['number_of_reviews'], bins=30, kde=True)
plt.title('Distribution of Number of Reviews')
plt.xlabel('Number of Reviews')
plt.ylabel('Frequency')
plt.show()

# Correlation heatmap for numerical variables
numerical_cols = ['days', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews']
sns.heatmap(airbnb_eda[numerical_cols].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Scatter plot of price vs. number of reviews
sns.scatterplot(x='price', y='number_of_reviews', data=airbnb_eda)
plt.title('Price vs. Number of Reviews')
plt.xlabel('Price')
plt.ylabel('Number of Reviews')
plt.show()

# Step 2: Clean data by dropping rows with missing values in model inputs
relevant_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms']
airbnb_cleaned = airbnb.dropna(subset=relevant_cols)

# Step 3: Build a Poisson Regression Model

# Define dependent and independent variables
X = airbnb_cleaned[['price', 'bathrooms', 'bedrooms']]
X = sm.add_constant(X)  # Add intercept
y = airbnb_cleaned['number_of_reviews']

# Fit the Poisson regression model
poisson_model = GLM(y, X, family=Poisson()).fit()

# Step 4: Interpret Model Coefficients
print(poisson_model.summary())

```

```{python}
airbnb_eda['intercept'] = 1
X = airbnb_eda[['intercept','price', 'bathrooms', 'bedrooms']].astype(float).to_numpy()
y = airbnb_eda['number_of_reviews'].values

initial_beta = np.zeros(X.shape[1])
result = minimize(
    fun=lambda b: -poisson_log_likelihood_regression(b, X, y),
    x0=initial_beta,
    method='BFGS'
)
beta_mle = result.x
log_lik_at_mle = -result.fun
hessian_inv = result.hess_inv
standard_errors = np.sqrt(np.diag(hessian_inv))

coef_df = pd.DataFrame({
    'Variable': ['intercept','price', 'bathrooms', 'bedrooms'],
    'Estimate (MLE)': [f"{val:.4f}" for val in beta_mle],
    'Std. Error': [f"{se:.4f}" for se in standard_errors]
})
```
```{python}
# | echo: false
# Display formatted HTML table
display(HTML(coef_df.to_html(index=False)))

# Display log-likelihood box
HTML(f"""
<div class='result-box'>
  <p><strong>Log-Likelihood at MLE:</strong> {log_lik_at_mle:.2f}</p>
</div>
""")
```

In this Poisson regression model, each additional bedroom is associated with a 9.3% increase in expected reviews. Bathrooms have a negative effect, with each additional bathroom linked to a 14% decrease. Price has a statistically significant but negligible impact, increasing expected reviews by just 0.02% per dollar.