---
title: "Multinomial Logit Model"
author: "Lowell C"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
```{r}
# set seed for reproducibility
set.seed(123)

# define attributes
brand <- c("N", "P", "H") # Netflix, Prime, Hulu
ad <- c("Yes", "No")
price <- seq(8, 32, by=4)

# generate all possible profiles
profiles <- expand.grid(
    brand = brand,
    ad = ad,
    price = price
)
m <- nrow(profiles)

# assign part-worth utilities (true parameters)
b_util <- c(N = 1.0, P = 0.5, H = 0)
a_util <- c(Yes = -0.8, No = 0.0)
p_util <- function(p) -0.1 * p

# number of respondents, choice tasks, and alternatives per task
n_peeps <- 100
n_tasks <- 10
n_alts <- 3

# function to simulate one respondentâ€™s data
sim_one <- function(id) {
  
    datlist <- list()
    
    # loop over choice tasks
    for (t in 1:n_tasks) {
        
        # randomly sample 3 alts (better practice would be to use a design)
        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])
        
        # compute deterministic portion of utility
        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)
        
        # add Gumbel noise (Type I extreme value)
        dat$e <- -log(-log(runif(n_alts)))
        dat$u <- dat$v + dat$e
        
        # identify chosen alternative
        dat$choice <- as.integer(dat$u == max(dat$u))
        
        # store task
        datlist[[t]] <- dat
    }
    
    # combine all tasks for one respondent
    do.call(rbind, datlist)
}

# simulate data for all respondents
conjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))

# remove values unobservable to the researcher
conjoint_data <- conjoint_data[ , c("resp", "task", "brand", "ad", "price", "choice")]

# clean up
rm(list=setdiff(ls(), "conjoint_data"))
```
::::



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.


_todo: reshape and prep the data_

```{python}
import pandas as pd
import numpy as np
```


```{python}
data = pd.read_csv('conjoint_data.csv')
data.head()

data = pd.get_dummies(data,columns=['brand','ad'],drop_first=True)
data.head()
```


## 4. Estimation via Maximum Likelihood

_todo: Code up the log-likelihood function._

```{python}

def log_likelihood(beta, X, y, group_indicator):
    utilities = np.dot(X, beta)         # ensure matrix multiplication
    utilities = np.asarray(utilities, dtype=np.float64)  # enforce numeric array
    exp_utilities = np.exp(utilities)   # now safe

    task_sums = group_indicator.T @ exp_utilities
    denom_per_row = group_indicator @ task_sums
    probs = exp_utilities / (denom_per_row + 1e-10)
    log_probs = np.log(probs + 1e-10)

    return -np.sum(y * log_probs)



```


_todo: Use `optim()` in R or `scipy.optimize()` in Python to find the MLEs for the 4 parameters ($\beta_\text{netflix}$, $\beta_\text{prime}$, $\beta_\text{ads}$, $\beta_\text{price}$), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval._


```{python}
import numpy as np

X = data[['brand_N', 'brand_P', 'ad_Yes', 'price']].values
y =data['choice'].values

# Step 1: Get unique task identifiers (e.g., (resp, task) pairs)
groups = data[['resp', 'task']].drop_duplicates().reset_index(drop=True)

# Step 2: Build a zero matrix: rows = alternatives, columns = tasks
n_rows = data.shape[0]
n_tasks = groups.shape[0]
group_indicator = np.zeros((n_rows, n_tasks))

# Step 3: Fill in 1s where each row belongs to a specific task
for task_index, (resp_id, task_id) in groups.iterrows():
    mask = (data['resp'] == resp_id) & (data['task'] == task_id)
    group_indicator[mask.values, task_index] = 1


beta_init = np.zeros(X.shape[1])  # often start with all 0s

from scipy.optimize import minimize

result = minimize(
    fun=log_likelihood,
    x0=beta_init,
    args=(X, y, group_indicator),
    method='BFGS'
)


beta_mle = result.x  # MLE estimates
hessian_inv = result.hess_inv  # estimated inverse Hessian
standard_errors = np.sqrt(np.diag(hessian_inv))  # approximate SEs

```
```{python}
import pandas as pd
import numpy as np

# 95% confidence intervals
conf_ints = np.column_stack([
    beta_mle - 1.96 * standard_errors,
    beta_mle + 1.96 * standard_errors
])

# Feature names (ensure they match the columns in X)
feature_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']

# Create results DataFrame
results_df = pd.DataFrame({
    'Coefficient': beta_mle,
    'Std. Error': standard_errors,
    'CI Lower (95%)': conf_ints[:, 0],
    'CI Upper (95%)': conf_ints[:, 1]
}, index=feature_names)

# Display table
print(results_df)

```

## 5. Estimation via Bayesian Methods

_todo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000._

_hint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta._

_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_

_hint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands.  Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous.  So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal.  Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005)._


_todo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution._

_todo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach._


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure numeric stability and float64 types
X = X.astype(np.float64)
y = y.astype(np.float64)
group_indicator = group_indicator.astype(np.float64)

# Modified log-posterior function: log-likelihood + log-prior
def log_posterior(beta, X, y, group_indicator):
    # log-likelihood
    utilities = np.dot(X, beta)
    exp_utilities = np.exp(utilities)
    task_sums = group_indicator.T @ exp_utilities
    denom_per_row = group_indicator @ task_sums
    probs = exp_utilities / (denom_per_row + 1e-10)
    log_probs = np.log(probs + 1e-10)
    log_lik = np.sum(y * log_probs)

    # log-prior
    # priors: N(0, 5) for first 3 (binary vars), N(0, 1) for price
    log_prior = (
        -0.5 * (beta[0]**2 / 5)
        -0.5 * (beta[1]**2 / 5)
        -0.5 * (beta[2]**2 / 5)
        -0.5 * (beta[3]**2 / 1)
    )

    return log_lik + log_prior

# MCMC settings
n_iter = 11000
n_burn = 1000
n_params = X.shape[1]
current_beta = np.zeros(n_params)
samples = np.zeros((n_iter, n_params))
current_log_post = log_posterior(current_beta, X, y, group_indicator)

# Proposal standard deviations
proposal_sds = np.array([0.05, 0.05, 0.05, 0.005])

# Metropolis-Hastings sampler
for i in range(n_iter):
    proposal = current_beta + np.random.normal(0, proposal_sds)
    proposal_log_post = log_posterior(proposal, X, y, group_indicator)

    # Accept/reject
    accept_prob = np.exp(proposal_log_post - current_log_post)
    if np.random.rand() < accept_prob:
        current_beta = proposal
        current_log_post = proposal_log_post

    samples[i] = current_beta

# Discard burn-in
samples_post_burn = samples[n_burn:]

# Posterior summaries
posterior_means = samples_post_burn.mean(axis=0)
posterior_sds = samples_post_burn.std(axis=0)
posterior_cis = np.percentile(samples_post_burn, [2.5, 97.5], axis=0).T

# Results DataFrame
param_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']
posterior_summary = pd.DataFrame({
    'Posterior Mean': posterior_means,
    'Posterior Std': posterior_sds,
    '95% CI Lower': posterior_cis[:, 0],
    '95% CI Upper': posterior_cis[:, 1]
}, index=param_names)


print(posterior_summary)

# Plot for price parameter (index 3)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(samples_post_burn[:, 3])
plt.title('Trace Plot for Î²_price')
plt.xlabel('Iteration')
plt.ylabel('Î²_price')

plt.subplot(1, 2, 2)
plt.hist(samples_post_burn[:, 3], bins=30, edgecolor='k')
plt.title('Posterior Histogram for Î²_price')
plt.xlabel('Î²_price')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()
```

## 6. Discussion

_todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does $\beta_\text{Netflix} > \beta_\text{Prime}$ mean? Does it make sense that $\beta_\text{price}$ is negative?_

_todo: At a high level, discuss what change you would need to make in order to simulate data from --- and estimate the parameters of --- a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze "real world" conjoint data._






Suppose you did not simulate the data. What do you observe about the parameter estimates? What does it mean if the Netflix coefficient is larger than the Prime coefficient? Does it make sense that the price coefficient is negative?

If the data were not simulated, the parameter estimates would be interpreted directly based on their signs, magnitudes, and significance. For example, a positive coefficient for the brand_N variable would suggest that respondents prefer Netflix over the baseline brand (such as Hulu). If the coefficient for Netflix is greater than the coefficient for Amazon Prime, it indicates that, on average, respondents derive more utility from choosing Netflix than Prime, all else being equal.

A negative coefficient for price makes intuitive sense â€” it reflects that as the price of an option increases, the likelihood of it being chosen decreases. This is consistent with standard economic theory and suggests that the model is capturing realistic price sensitivity.

At a high level, what would need to change in order to simulate and estimate a multi-level (random-parameter or hierarchical) model?

In a basic model, we assume that all respondents share the same set of preferences (i.e., the same coefficients). In a hierarchical model, we instead allow each respondent to have their own set of coefficients. These individual-level coefficients are drawn from a population-level distribution with a mean and variance that we estimate.

To simulate such data, you would:

Draw individual preference vectors (coefficients) for each respondent from a normal distribution centered around a population mean, with some standard deviation to introduce heterogeneity.
Use those individual-level preferences to generate choices for each task.
To estimate this type of model, you would:

Estimate both the population-level parameters (mean and variance of preferences) and the individual-level parameters (one set of coefficients per respondent).
Use methods like Hierarchical Bayes (HB) or mixed logit models that rely on simulation or MCMC to estimate the distribution of preferences.
This approach provides richer insights into how preferences vary across individuals and is commonly used in real-world conjoint studies to enable better targeting and segmentation.
